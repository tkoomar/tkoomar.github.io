[{"authors":["tanner"],"categories":null,"content":"I study the genetic intersection of language development and the cognitive functions underlying neuropsychiatric conditions. I rely on a variety of computational and statistical tools like machine learning and cloud infrastructure. I am also passionate about communicating science and the duty of scientists to thoughtfully share their research the rest of society.\n","date":1615248000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1615266000,"objectID":"e83c65b8f9f38577f6c5ff18ba0e6f97","permalink":"/authors/tanner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tanner/","section":"authors","summary":"I study the genetic intersection of language development and the cognitive functions underlying neuropsychiatric conditions. I rely on a variety of computational and statistical tools like machine learning and cloud infrastructure. I am also passionate about communicating science and the duty of scientists to thoughtfully share their research the rest of society.","tags":null,"title":"Tanner Koomar","type":"authors"},{"authors":["Tanner Koomar"],"categories":["tidytuesday"],"content":"  Load Packages Read in data Top Plot Annotate it  Bottom Plot Plots Assemble!   Load Packages library(tidyverse) library(patchwork) library(showtext) font_add_google(\u0026quot;Righteous\u0026quot;, \u0026quot;Righteous\u0026quot;) showtext_auto()  Read in data There’s a miscalculation where the obscure Frozen (2010) has the same reported box office as the Disney musical Frozen (2013). I’m too lazy to look up what its real box office was, so I just removed it.\nmovies_detail \u0026lt;- readr::read_csv(\u0026#39;https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-03-09/movies.csv?raw=true\u0026#39;, na = c(\u0026quot;NA\u0026quot;, \u0026quot;#N/A\u0026quot;)) %\u0026gt;% mutate(binary = binary == \u0026quot;PASS\u0026quot;) %\u0026gt;% filter(!(year == 2010 \u0026amp; title == \u0026quot;Frozen\u0026quot;)) movies \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-09/raw_bechdel.csv\u0026#39;)   Top Plot top_pct_lustrum \u0026lt;- movies_detail %\u0026gt;% drop_na(domgross_2013) %\u0026gt;% mutate(lustrum = cut_width(year, 5, boundary = 1975)) %\u0026gt;% group_by(lustrum) %\u0026gt;% arrange(desc(domgross_2013)) %\u0026gt;% mutate(rank = rank(domgross_2013)) %\u0026gt;% mutate(rank = rank/max(rank)) %\u0026gt;% mutate(pct_gross = domgross_2013/sum(domgross_2013)) %\u0026gt;% filter(rank \u0026gt;= 0.9) %\u0026gt;% summarize( pct_lustrum = mean(binary, na.rm =T), pct_lustrum_gross = sum(pct_gross) ) %\u0026gt;% ungroup() top_pct_year \u0026lt;- movies_detail %\u0026gt;% drop_na(domgross_2013) %\u0026gt;% mutate(lustrum = cut_width(year, 5, boundary = 1975)) %\u0026gt;% group_by(year, lustrum) %\u0026gt;% mutate(rank = rank(domgross_2013)) %\u0026gt;% mutate(rank = rank/max(rank)) %\u0026gt;% mutate(pct_gross = domgross_2013/sum(domgross_2013)) %\u0026gt;% filter(rank \u0026gt;= 0.75) %\u0026gt;% summarize( pct_year = mean(binary, na.rm =T), pct_year_gross = sum(pct_gross) ) %\u0026gt;% ungroup() p1 \u0026lt;- top_pct_lustrum %\u0026gt;% inner_join(top_pct_year) %\u0026gt;% ggplot(aes(x = year)) + geom_col( aes(y = pct_lustrum_gross), fill = \u0026#39;grey90\u0026#39;, width = 1, alpha = 1/2 ) + geom_point(aes(y = pct_year), shape = 3, size = 3, color = \u0026#39;grey70\u0026#39;) + geom_line(aes(y = pct_lustrum), color = \u0026#39;grey20\u0026#39;) + theme_minimal(base_size = 14, base_family = \u0026quot;sans\u0026quot;) + scale_x_continuous(expand = expansion(add = c(1,5))) + scale_y_continuous(labels = scales::label_percent()) + labs( y = NULL, x = \u0026#39;top grossing movies of half-decade (bold pass test)\u0026#39;) + theme( axis.title = element_text( family = \u0026quot;Righteous\u0026quot;, face = \u0026#39;bold\u0026#39;, hjust = 0, size = 16, color = \u0026#39;orangered2\u0026#39;, ), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank() ) p1 Annotate it p1 \u0026lt;- p1 + annotate( geom = \u0026#39;curve\u0026#39;, xend = 1984.75, yend = .01, x = 1982, y = 0.5, curvature = 0.15,ncp = 9, lwd = 1/2, arrow = arrow(length = unit(1/2, \u0026#39;lines\u0026#39;)), color = \u0026#39;orangered1\u0026#39; ) + annotate( geom = \u0026#39;text\u0026#39;, family = \u0026quot;Righteous\u0026quot;, x = 1981, y = 0.51, size= 4.25, label = \u0026quot;The year Alison Bechdel proposed her now famous test was in the midst of a decade-long drought for female representation in popular movies\u0026quot; %\u0026gt;% str_wrap(30), hjust = 0, vjust = 0, color = \u0026#39;orangered1\u0026#39; ) + geom_vline( xintercept = 1980, color = \u0026#39;grey80\u0026#39;, alpha = 1/2, lty = 2, size = 1 ) + annotate( geom = \u0026#39;text\u0026#39;, x = 1979.85, y = 0.80, label = \u0026#39;data before this date is sparse and less accurate\u0026#39;, angle = 90, hjust = 1/2, vjust = 0, size = 2.75, color = \u0026#39;grey80\u0026#39; ) + annotate( geom = \u0026#39;text\u0026#39;, x = 1970.05, y = .358, hjust = 0, vjust = 1, color = \u0026#39;grey30\u0026#39;, size = 2.75, label = \u0026#39;top 10% grossing movies in \\nhalf-decade which pass test\u0026#39; ) + annotate( geom = \u0026#39;curve\u0026#39;, xend = 1970.2, yend = .98, x = 1971, y = 0.9, curvature = 0.15, ncp = 9, lwd = 1/2, arrow = arrow(length = unit(1/3, \u0026#39;lines\u0026#39;)), color = \u0026#39;grey70\u0026#39; ) + annotate( geom = \u0026#39;text\u0026#39;, x = 1971, y = .89, hjust = 0, vjust = 1, color = \u0026#39;grey70\u0026#39;, size = 2.75, label = \u0026#39;top 50% grossing movies in\\nyear which pass test\u0026#39; ) + annotate( geom = \u0026#39;text\u0026#39;, color = \u0026#39;grey75\u0026#39;, x = 1975.75, y = 0.46, hjust = 0, vjust = 1, size = 2.5, label = \u0026#39;percent USA box office for\\ntop 10% grossing movies\u0026#39; ) p1   Bottom Plot p2 \u0026lt;- movies_detail %\u0026gt;% drop_na(domgross_2013) %\u0026gt;% mutate(lustrum = cut_width(year, 5, boundary = 1975)) %\u0026gt;% group_by(lustrum) %\u0026gt;% mutate(rank = rank(domgross_2013)) %\u0026gt;% mutate(rank = rank/max(rank)) %\u0026gt;% arrange(desc(rank)) %\u0026gt;% slice(1:5) %\u0026gt;% select(lustrum, title, binary) %\u0026gt;% mutate( title = title %\u0026gt;% str_replace(\u0026quot;\u0026amp;#39;\u0026quot;, \u0026quot;\u0026#39;\u0026quot;) %\u0026gt;% str_remove_all(\u0026quot;Star Wars: Episode .*- \u0026quot;) %\u0026gt;% str_remove_all(\u0026quot;The Lord of the Rings: \u0026quot;) ) %\u0026gt;% mutate( rank = rank(str_length(title), ties.method = \u0026#39;first\u0026#39;), title = str_wrap(title, width = 25) ) %\u0026gt;% ggplot(aes(x = lustrum, y = rank)) + geom_text(aes(label = title, color = binary), hjust = 0, vjust = 1, size = 3, lineheight = 3/4) + theme_void(base_family = \u0026quot;sans\u0026quot;) + scale_x_discrete(expand = expansion(add = c(1/5,8/5))) + scale_y_discrete(expand = expansion(add = c(3/5,1/5))) + scale_color_manual( values = c(\u0026#39;TRUE\u0026#39; = \u0026#39;grey20\u0026#39;, \u0026#39;FALSE\u0026#39; = \u0026#39;grey65\u0026#39;), guide = guide_none()) + labs( caption = \u0026quot;A #TidyTuesday adventure by @tannerkoomar\\nData from @FiveThirtyEight\u0026quot; ) p2  Plots Assemble! final_plot \u0026lt;- wrap_plots( list(p1, p2), ncol = 1, heights = c(4, 1) ) final_plot  ","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615266000,"objectID":"88169c31fbbccad9272490035a97c713","permalink":"/post/tt-bechdel/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/post/tt-bechdel/","section":"post","summary":"A look at female representation in the top grossing movies of the last 40 years. (Spoiler: it isn't great)","tags":["R","data visualization"],"title":"Bechdel Test","type":"post"},{"authors":["Tanner Koomar"],"categories":["tidytuesday"],"content":"  Setup Peek at the data Get a Map Combine Map and Data Combine Rehomed and Released   Setup library(tidyverse) library(sf) library(ggtext) library(patchwork) library(showtext) font_add_google(\u0026quot;Josefin Sans\u0026quot;, \u0026quot;Josefin Sans\u0026quot;) showtext_auto() animal_outcomes \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-21/animal_outcomes.csv\u0026#39;) %\u0026gt;% filter(year \u0026lt;= 2015)  Peek at the data head(animal_outcomes) ## # A tibble: 6 x 12 ## year animal_type outcome ACT NSW NT QLD SA TAS VIC WA ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1999 Dogs Reclai… 610 3140 205 1392 2329 516 7130 1 ## 2 1999 Dogs Rehomed 1245 7525 526 5489 1105 480 4908 137 ## 3 1999 Dogs Other 12 745 955 860 380 168 1001 6 ## 4 1999 Dogs Euthan… 360 9221 9 9214 1701 599 5217 18 ## 5 1999 Cats Reclai… 111 201 22 206 157 31 884 0 ## 6 1999 Cats Rehomed 1442 3913 269 3901 1055 752 3768 62 ## # … with 1 more variable: Total \u0026lt;dbl\u0026gt;  Get a Map au_sf \u0026lt;- rnaturalearth::ne_states(country = \u0026#39;Australia\u0026#39;, returnclass = \u0026#39;sf\u0026#39;) %\u0026gt;% mutate(state = str_remove(iso_3166_2, \u0026quot;AU-\u0026quot;)) ggplot(au_sf) + geom_sf() + theme_minimal()  Get colors Make a vector of colors for each state/territory.\nstate_colors \u0026lt;- au_sf %\u0026gt;% filter(state %in% colnames(animal_outcomes) \u0026amp; !is.na(abbrev)) %\u0026gt;% as.data.frame() %\u0026gt;% transmute(name = name, state = state, color = wesanderson::wes_palette(\u0026quot;BottleRocket2\u0026quot;, n = 12, type = \u0026#39;c\u0026#39;)[c(1:6,9,12)] %\u0026gt;% colorspace::lighten(amount = .2), name_md = glue::glue(\u0026quot;\u0026lt;b style=\u0026#39;color:{color}\u0026#39;\u0026gt;{name}\u0026lt;/b\u0026gt;\u0026quot;) )    Combine Map and Data Because the states / territories need to go into the rows (sf has 1 row per geometry), we will be pivoting the data longer before we join.\nanimal_outcomes_long \u0026lt;- animal_outcomes %\u0026gt;% select(-Total) %\u0026gt;% pivot_longer(cols = 4:11, names_to = \u0026#39;state\u0026#39;, values_to = \u0026#39;count\u0026#39;) %\u0026gt;% pivot_wider(names_from = outcome, values_from = count) %\u0026gt;% mutate_all(replace_na, 0) %\u0026gt;% inner_join(state_colors, by = \u0026#39;state\u0026#39;)  Combine Rehomed and Released Wild animals are obviously released — rather than rehomed — but both can be viewed as the alternative to being euthanized, so we will combine them. Really not sure what “Other” would mean, so let’s just exclude it.\nanimal_outcomes_long \u0026lt;- animal_outcomes_long %\u0026gt;% mutate(`Not Euthanized` = (Reclaimed + Rehomed + Released), ratio = (`Not Euthanized` + 1) / (Euthanized + 1) ) animal_outcomes_long[1:5,1:8] ## # A tibble: 5 x 8 ## year animal_type state Reclaimed Rehomed Other Euthanized Released ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1999 Dogs ACT 610 1245 12 360 0 ## 2 1999 Dogs NSW 3140 7525 745 9221 0 ## 3 1999 Dogs NT 205 526 955 9 0 ## 4 1999 Dogs QLD 1392 5489 860 9214 0 ## 5 1999 Dogs SA 2329 1105 380 1701 0 heatmap_plot \u0026lt;- animal_outcomes_long %\u0026gt;% ggplot(aes(x = year, y = name_md, fill = ratio)) + geom_tile() + scale_fill_fermenter(palette = \u0026#39;Spectral\u0026#39;, limits = c(1/16, 16), breaks = c(1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16), trans = \u0026#39;log2\u0026#39;, direction = 8, show.limits = F, labels = c(\u0026#39;More\\nEuthanizations\u0026#39;, \u0026#39;8 x\u0026#39;, \u0026#39;4 x\u0026#39;, \u0026#39;2 x\u0026#39;, \u0026#39;1 x\u0026#39;, \u0026#39;2 x\u0026#39;, \u0026#39;4 x\u0026#39;, \u0026#39;8 x\u0026#39;, \u0026#39;More\\nRehomes \u0026amp; Releases\u0026#39;), guide = guide_colorsteps(title = NULL) ) + scale_x_continuous(expand = c(0,0)) + facet_wrap(~animal_type, ncol = 3) + labs(title = \u0026quot;Breakdown by Animal Type\u0026quot;) + theme_bw(base_size = 16) + theme( plot.title = element_text(hjust = 0.5), plot.margin = margin(t = 25, b = 0), legend.box.margin = margin(), legend.margin = margin(b = -10), legend.position = \u0026#39;top\u0026#39;, axis.text.y.left = element_markdown(size = 12), panel.border = element_blank(), line = element_blank(), axis.title = element_blank(), axis.ticks.length = unit(0, \u0026#39;pt\u0026#39;), legend.key.width = unit(100, \u0026#39;pt\u0026#39;), legend.key.height = unit(10, \u0026#39;pt\u0026#39;), strip.background = element_rect(color = NA, fill = \u0026#39;grey95\u0026#39;) ) heatmap_plot au_map \u0026lt;- au_sf %\u0026gt;% filter(state %in% colnames(animal_outcomes) \u0026amp; !is.na(abbrev)) %\u0026gt;% ggplot(aes(fill = name)) + geom_sf(color= \u0026#39;grey40\u0026#39;, size =1/8) + scale_fill_manual(values = state_colors %\u0026gt;% select(name, color) %\u0026gt;% deframe(), guide = guide_none()) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;, legend.key = element_blank(), plot.margin = margin() ) au_map line_plot \u0026lt;- animal_outcomes_long %\u0026gt;% group_by(name, year, name_md) %\u0026gt;% summarize(\u0026#39;Not Euthanized\u0026#39; = sum(`Not Euthanized`), Euthanized = sum(Euthanized), ratio = (`Not Euthanized` + 1) / (Euthanized + 1), .groups = \u0026#39;keep\u0026#39;) %\u0026gt;% ggplot(aes(x = year, y = ratio, color = name_md)) + geom_hline(yintercept = 1, color = \u0026#39;grey35\u0026#39;, size = 1) + geom_smooth(alpha = 0, size = 1.75) + scale_color_manual(values = state_colors %\u0026gt;% select(name_md, color) %\u0026gt;% deframe() %\u0026gt;% alpha(0.85), name = \u0026#39;State or Territory\u0026#39;) + scale_y_continuous(limits = c(1/2, 16), breaks = c(1/2, 1, 2, 4, 8, 16), trans = \u0026#39;log2\u0026#39;, labels = c(\u0026#39;More\\nEuthanizations\u0026#39;, \u0026#39;1 x\u0026#39;, \u0026#39;2 x\u0026#39;, \u0026#39;4 x\u0026#39;, \u0026#39;8 x\u0026#39;, \u0026#39;More\\nRehomes\u0026#39;), position = \u0026#39;right\u0026#39;) + labs(title = \u0026#39;Total\u0026#39;) + theme_bw(base_size = 14) + theme( plot.title.position = \u0026#39;panel\u0026#39;, plot.margin = margin(), aspect.ratio = 1, panel.grid.major = element_line(size = 0.35, color = \u0026#39;grey70\u0026#39;), panel.border = element_blank(), legend.text = element_markdown(), legend.key = element_blank(), axis.title = element_blank(), axis.ticks.length = unit(0, \u0026#39;pt\u0026#39;), legend.key.width = unit(0, \u0026#39;pt\u0026#39;), legend.key.height = unit(0, \u0026#39;pt\u0026#39;), legend.position = \u0026#39;left\u0026#39; ) line_plot ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; ## Warning: Removed 3 rows containing non-finite values (stat_smooth). showtext_auto() design \u0026lt;- \u0026quot; AABBBB AABBBB CDDDDD CDDDDD CDDDDD \u0026quot; final_plot \u0026lt;- au_map + line_plot + plot_spacer() + heatmap_plot + plot_layout(design = design ) + plot_annotation(title = \u0026#39;Animal Rehomes \u0026amp; Releases in Australia\u0026#39;, caption = \u0026#39;A #tidytuesday adventure\\nDesign by @TannerKoomar\\nData from RSPCA\u0026#39;, subtitle = \u0026#39;1999 - 2015\u0026#39;, theme = theme(title = element_text(size = 27, color = \u0026#39;grey40\u0026#39;, family = \u0026#39;Josefin Sans\u0026#39;), plot.subtitle = element_text(size = 20), plot.caption = element_text(size = 10), text = element_text(size = 10) ) ) + theme(axis.text= element_text(size = 8)) \u0026amp; theme(text = element_text(color = \u0026#39;grey40\u0026#39;, family = \u0026#39;Josefin Sans\u0026#39;), strip.text = element_text(color = \u0026#39;grey40\u0026#39;, family = \u0026#39;Josefin Sans\u0026#39;), legend.title = element_text(color = \u0026#39;grey40\u0026#39;, family = \u0026#39;Josefin Sans\u0026#39;) ) final_plot ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; ## Warning: Removed 3 rows containing non-finite values (stat_smooth).  ","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595280902,"objectID":"d2bccc481b8dd3a0ec6eb5192c333102","permalink":"/post/2020-07-20-tt-australian-pets/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/post/2020-07-20-tt-australian-pets/","section":"post","summary":"Since ~2007, the RSPCA has seen increased success rehoming and releasing animals","tags":["R","data visualization","heatmap","map"],"title":"Australian Pets","type":"post"},{"authors":["Tanner Koomar"],"categories":null,"content":"","date":1591228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591228800,"objectID":"a84af805bf3e2b2be4b99b3a20d95965","permalink":"/publication/2020-vcfdbr/","publishdate":"2020-06-04T00:00:00Z","relpermalink":"/publication/2020-vcfdbr/","section":"publication","summary":"As exome and whole-genome sequencing cohorts grow in size, the data they produce strains the limits of current tools and data structures. The Variant Call Format (VCF) was originally created as part of the 1,000 Genomes project. Flexible and concise enough to describe the genetic variations of thousands of samples in a single flat file, the VCF has become the standard for communicating the results of large-scale sequencing experiments. Because of its static and text-based structure, VCFs remain cumbersome to parse and filter in an interactive way, even with the aid of indexing. Iterating on previous concepts, we propose here a pipeline for converting VCFs to simple SQLite databases, which allow for rapid searching and filtering of genetic variants while minimizing memory overhead. Code can be found at https://github.com/tkoomar/VCFdbR","tags":["genetics","whole genome sequencing","vcf"],"title":"VCFdbR: A method for expressing biobank-scale Variant Call Format data in a SQLite database using R","type":"publication"},{"authors":null,"categories":["tutorials"],"content":"Note that this is not without shortcomings and lacks some flexibility, but it does keep you from having dig deep into how formulas actually work in R – which many (myself included) find confusing in the extreme.\nFor this example, let’s work with the standard mtcars dataset.\nhead(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 The function, lm_caller() The function we will make – lm_caller() – is rather dull at its core. It just takes in different arguments for the left-hand side (LHS), right-hand side (RHS) and data of an model you want to fit.\nlm_caller \u0026lt;- function(LHS, RHS, data){ formula \u0026lt;- as.formula(paste0(LHS, \u0026quot; ~ \u0026quot;, RHS)) output \u0026lt;- lm(formula, data = data) return(output) }  Run the function manually model_1 \u0026lt;- lm_caller(LHS = \u0026quot;mpg\u0026quot;, RHS = \u0026quot;disp + wt\u0026quot;, data = mtcars) summary(model_1) ## ## Call: ## lm(formula = formula, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10  Iterate (apply) over many dependent and independent variables First, make a big ol’ list of the different combinations of variables you want in your models: left_hand_side \u0026lt;- data.frame(lhs = c(\u0026#39;mpg\u0026#39;, \u0026#39;hp\u0026#39;, \u0026#39;am\u0026#39;)) right_hand_side \u0026lt;- data.frame(rhs = c(\u0026#39;cyl\u0026#39;, \u0026#39;disp\u0026#39;, \u0026#39;wt\u0026#39;, \u0026#39;qsec\u0026#39;, \u0026#39;disp + wt\u0026#39;, \u0026#39;cyl - 1\u0026#39;))  Now do a nested apply() over each of those model_list \u0026lt;- apply(left_hand_side, 1, function(lhs){ apply(right_hand_side, 1,function(rhs){ lm_caller(LHS = lhs, RHS = rhs, data = mtcars) }) }) One major downside to munging strings for model specification is that the formula stored in the resulting models will all look identical:\nmodel_list[[1]][1:2] ## [[1]] ## ## Call: ## lm(formula = formula, data = data) ## ## Coefficients: ## (Intercept) cyl ## 37.885 -2.876 ## ## ## [[2]] ## ## Call: ## lm(formula = formula, data = data) ## ## Coefficients: ## (Intercept) disp ## 29.59985 -0.04122 Mapping these models back to their original data is a bit of a pain. There is a way to do it that I find much cleaner, so long as you are willing to. . .\n Do it tidy! If this approach make sense to you, I would recommend this basic tutorial on functional programming with purrr in R and this book chapter on building many models, utilizing the power of the tidyverse\nFirst, make a data frame of all the left- and right-hand sides, so we can keep track of them:\nformulae_dat \u0026lt;- tidyr::crossing(left_hand_side, right_hand_side) head(formulae_dat) ## # A tibble: 6 x 2 ## lhs rhs ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 am cyl ## 2 am cyl - 1 ## 3 am disp ## 4 am disp + wt ## 5 am qsec ## 6 am wt Now, add the models to this, with the help of mutate() and map2\nNote we could also change up the dataset as part of this (not sure why you’d want to), but that would require using pmap() instead of map2\nmodel_dat \u0026lt;- dplyr::mutate(formulae_dat, model = purrr::map2(lhs, rhs, function(lhs, rhs){ lm_caller(LHS = lhs, RHS = rhs, data = mtcars) }) ) head(model_dat) ## # A tibble: 6 x 3 ## lhs rhs model ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; ## 1 am cyl \u0026lt;lm\u0026gt; ## 2 am cyl - 1 \u0026lt;lm\u0026gt; ## 3 am disp \u0026lt;lm\u0026gt; ## 4 am disp + wt \u0026lt;lm\u0026gt; ## 5 am qsec \u0026lt;lm\u0026gt; ## 6 am wt \u0026lt;lm\u0026gt; Having models inside of a dataframe might not seem any more helpful, but you can write a function to extract the parts that you really want. Or, rely on broom for more common models.\nNote here I’m using lambda style function specification, rather than the more verbose function(x){} format.\nmodel_dat \u0026lt;- dplyr::mutate(model_dat, model_summary = purrr::map(model, ~broom::glance(.x)), model_coefficient = purrr::map(model, ~broom::tidy(.x)) ) head(model_dat) ## # A tibble: 6 x 5 ## lhs rhs model model_summary model_coefficient ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 am cyl \u0026lt;lm\u0026gt; \u0026lt;tibble [1 × 11]\u0026gt; \u0026lt;tibble [2 × 5]\u0026gt; ## 2 am cyl - 1 \u0026lt;lm\u0026gt; \u0026lt;tibble [1 × 11]\u0026gt; \u0026lt;tibble [1 × 5]\u0026gt; ## 3 am disp \u0026lt;lm\u0026gt; \u0026lt;tibble [1 × 11]\u0026gt; \u0026lt;tibble [2 × 5]\u0026gt; ## 4 am disp + wt \u0026lt;lm\u0026gt; \u0026lt;tibble [1 × 11]\u0026gt; \u0026lt;tibble [3 × 5]\u0026gt; ## 5 am qsec \u0026lt;lm\u0026gt; \u0026lt;tibble [1 × 11]\u0026gt; \u0026lt;tibble [2 × 5]\u0026gt; ## 6 am wt \u0026lt;lm\u0026gt; \u0026lt;tibble [1 × 11]\u0026gt; \u0026lt;tibble [2 × 5]\u0026gt; Now, with an unnest, we can access the most relevant parts of each model.\ntidyr::unnest(model_dat[,c(-3,-5)], model_summary) ## # A tibble: 18 x 13 ## lhs rhs r.squared adj.r.squared sigma statistic p.value df logLik ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 am cyl 0.273 0.249 0.432 11.3 2.15e- 3 2 -17.5 ## 2 am cyl … 0.253 0.229 0.560 10.5 2.84e- 3 1 -26.3 ## 3 am disp 0.350 0.328 0.409 16.1 3.66e- 4 2 -15.8 ## 4 am disp… 0.482 0.446 0.371 13.5 7.17e- 5 3 -12.1 ## 5 am qsec 0.0528 0.0213 0.494 1.67 2.06e- 1 2 -21.8 ## 6 am wt 0.480 0.462 0.366 27.6 1.13e- 5 2 -12.2 ## 7 hp cyl 0.693 0.683 38.6 67.7 3.48e- 9 2 -161. ## 8 hp cyl … 0.939 0.937 40.5 476. 2.23e-20 1 -163. ## 9 hp disp 0.626 0.613 42.6 50.1 7.14e- 8 2 -164. ## 10 hp disp… 0.635 0.609 42.9 25.2 4.57e- 7 3 -164. ## 11 hp qsec 0.502 0.485 49.2 30.2 5.77e- 6 2 -169. ## 12 hp wt 0.434 0.415 52.4 23.0 4.15e- 5 2 -171. ## 13 mpg cyl 0.726 0.717 3.21 79.6 6.11e-10 2 -81.7 ## 14 mpg cyl … 0.734 0.725 11.0 85.5 2.02e-10 1 -122. ## 15 mpg disp 0.718 0.709 3.25 76.5 9.38e-10 2 -82.1 ## 16 mpg disp… 0.781 0.766 2.92 51.7 2.74e-10 3 -78.1 ## 17 mpg qsec 0.175 0.148 5.56 6.38 1.71e- 2 2 -99.3 ## 18 mpg wt 0.753 0.745 3.05 91.4 1.29e-10 2 -80.0 ## # … with 4 more variables: AIC \u0026lt;dbl\u0026gt;, BIC \u0026lt;dbl\u0026gt;, deviance \u0026lt;dbl\u0026gt;, ## # df.residual \u0026lt;int\u0026gt; tidyr::unnest(model_dat[,-3:-4], model_coefficient) ## # A tibble: 36 x 7 ## lhs rhs term estimate std.error statistic p.value ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 am cyl (Intercept) 1.31 0.280 4.68 0.0000571 ## 2 am cyl cyl -0.146 0.0435 -3.36 0.00215 ## 3 am cyl - 1 cyl 0.0498 0.0154 3.24 0.00284 ## 4 am disp (Intercept) 0.955 0.155 6.18 0.000000855 ## 5 am disp disp -0.00238 0.000593 -4.02 0.000366 ## 6 am disp + wt (Intercept) 1.60 0.276 5.81 0.00000266 ## 7 am disp + wt disp 0.000451 0.00117 0.386 0.703 ## 8 am disp + wt wt -0.404 0.148 -2.73 0.0108 ## 9 am qsec (Intercept) 1.55 0.890 1.74 0.0914 ## 10 am qsec qsec -0.0642 0.0496 -1.29 0.206 ## # … with 26 more rows Functions like filter() and ggplot() are extremely useful at this point to get a beter feel for what your mnodels are actually saying, but we will save that for another time.\n  Plot from multiple models with ggplot library(tidyverse, quietly = T) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.0 ✓ purrr 0.3.3 ## ✓ tibble 3.0.0 ✓ dplyr 1.0.0 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() model_dat %\u0026gt;% unnest(model_coefficient) %\u0026gt;% mutate( low = estimate - std.error, high = estimate + std.error, formula = paste(lhs, \u0026quot;~\u0026quot;, rhs) ) %\u0026gt;% ggplot(aes(estimate, term, xmin = low, xmax = high, height = 0)) + geom_vline(xintercept = 0, color = \u0026#39;grey50\u0026#39;) + geom_errorbarh() + geom_point() + geom_text(aes(label = round(p.value, 3)), nudge_y = 0.33, ) + facet_wrap(vars(formula), scales = \u0026#39;free\u0026#39;, ncol = 2)  model_dat %\u0026gt;% unnest(model_coefficient) %\u0026gt;% filter(str_detect(rhs, \u0026quot;cyl\u0026quot;)) %\u0026gt;% mutate( low = estimate - std.error, high = estimate + std.error ) %\u0026gt;% ggplot(aes(estimate, term, xmin = low, xmax = high, height = 0)) + geom_vline(xintercept = 0, color = \u0026#39;grey50\u0026#39;) + geom_errorbarh() + geom_point() + geom_text(aes(label = round(p.value, 3)), nudge_y = 0.33, ) + facet_grid(rows = vars(lhs), cols = vars(rhs), scales = \u0026#39;free\u0026#39;)  Plotting the “acutal” vs “predicted” (fitted) values takes a bit of work with pivot_longer(), and is probably best done separately for each outcome variable.\nRemember that ggplots are actually objects, so we can actually stick them into a dataframe and then plot them later!\nplot_dat \u0026lt;- model_dat %\u0026gt;% mutate(model_resid = map(model, broom::augment)) %\u0026gt;% select(lhs, rhs, model_resid) %\u0026gt;% unnest(model_resid) %\u0026gt;% pivot_longer(cols = c(-starts_with(\u0026quot;.\u0026quot;), -lhs, -rhs), names_to = \u0026#39;name\u0026#39;, values_to =\u0026#39;actual\u0026#39; ) %\u0026gt;% filter(lhs == name) %\u0026gt;% nest(data = -lhs) %\u0026gt;% mutate(plot = map2(data, lhs, ~( ggplot(.x, aes(x = actual, y = .fitted, height = 0)) + geom_point() + facet_wrap(~rhs, scales = \u0026#39;free\u0026#39;) + labs(title = .y) ))) plot_dat ## # A tibble: 3 x 3 ## lhs data plot ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 am \u0026lt;tibble [192 × 11]\u0026gt; \u0026lt;gg\u0026gt; ## 2 hp \u0026lt;tibble [192 × 11]\u0026gt; \u0026lt;gg\u0026gt; ## 3 mpg \u0026lt;tibble [192 × 11]\u0026gt; \u0026lt;gg\u0026gt; plot_dat$plot ## [[1]] ## ## [[2]] ## ## [[3]]  ","date":1588377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588377600,"objectID":"ca97b93ad40fbce6e3aab156dbba1515","permalink":"/post/2020-05-02-tutorial-iterate-lm/","publishdate":"2020-05-02T00:00:00Z","relpermalink":"/post/2020-05-02-tutorial-iterate-lm/","section":"post","summary":"  The function, lm_caller() Run the function manually Iterate (apply) over many dependent and independent variables Do it tidy!  Plot from multiple models with ggplot   This short tutorial covers the basics of making and using a function that takes characters for dependent (LHS) and independent (RHS) variables of interest and converts them into a formula for use in modeling, such as with ‘lm()’.\n","tags":["lm","modeling","R","tidy","tutorial"],"title":"Simple Iteration over Variables with `lm()`","type":"post"},{"authors":null,"categories":["tidytuesday"],"content":"Dip into the new ggttext package to get some rich text formatting in a plot\u0026rsquo;s title. Also: sports.\n  Load in data   The question: to super bowl winners have harder schedules?   Summarize data   Test the difference in schedule strength   Plot   Bonus: Color by Team Name   Bonus: What is the bump in attendance the year after winning a superbowl?   Load in data attendance \u0026lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/attendance.csv', col_types = cols()) glimpse(attendance)  ## Observations: 10,846 ## Variables: 8 ## $ team \u0026lt;chr\u0026gt; \u0026quot;Arizona\u0026quot;, \u0026quot;Arizona\u0026quot;, \u0026quot;Arizona\u0026quot;, \u0026quot;Arizona\u0026quot;, \u0026quot;Arizon… ## $ team_name \u0026lt;chr\u0026gt; \u0026quot;Cardinals\u0026quot;, \u0026quot;Cardinals\u0026quot;, \u0026quot;Cardinals\u0026quot;, \u0026quot;Cardinals\u0026quot;,… ## $ year \u0026lt;dbl\u0026gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200… ## $ total \u0026lt;dbl\u0026gt; 893926, 893926, 893926, 893926, 893926, 893926, 893… ## $ home \u0026lt;dbl\u0026gt; 387475, 387475, 387475, 387475, 387475, 387475, 387… ## $ away \u0026lt;dbl\u0026gt; 506451, 506451, 506451, 506451, 506451, 506451, 506… ## $ week \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ weekly_attendance \u0026lt;dbl\u0026gt; 77434, 66009, NA, 71801, 66985, 44296, 38293, 62981…  standings \u0026lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-04/standings.csv', col_types = cols()) glimpse(standings)  ## Observations: 638 ## Variables: 15 ## $ team \u0026lt;chr\u0026gt; \u0026quot;Miami\u0026quot;, \u0026quot;Indianapolis\u0026quot;, \u0026quot;New York\u0026quot;, \u0026quot;Buffalo\u0026quot;, … ## $ team_name \u0026lt;chr\u0026gt; \u0026quot;Dolphins\u0026quot;, \u0026quot;Colts\u0026quot;, \u0026quot;Jets\u0026quot;, \u0026quot;Bills\u0026quot;, \u0026quot;Patriots\u0026quot;… ## $ year \u0026lt;dbl\u0026gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, … ## $ wins \u0026lt;dbl\u0026gt; 11, 10, 9, 8, 5, 13, 12, 9, 7, 4, 3, 12, 11, 7, … ## $ loss \u0026lt;dbl\u0026gt; 5, 6, 7, 8, 11, 3, 4, 7, 9, 12, 13, 4, 5, 9, 10,… ## $ points_for \u0026lt;dbl\u0026gt; 323, 429, 321, 315, 276, 346, 333, 321, 367, 185… ## $ points_against \u0026lt;dbl\u0026gt; 226, 326, 321, 350, 338, 191, 165, 255, 327, 359… ## $ points_differential \u0026lt;dbl\u0026gt; 97, 103, 0, -35, -62, 155, 168, 66, 40, -174, -2… ## $ margin_of_victory \u0026lt;dbl\u0026gt; 6.1, 6.4, 0.0, -2.2, -3.9, 9.7, 10.5, 4.1, 2.5, … ## $ strength_of_schedule \u0026lt;dbl\u0026gt; 1.0, 1.5, 3.5, 2.2, 1.4, -1.3, -2.5, -0.2, -1.4,… ## $ simple_rating \u0026lt;dbl\u0026gt; 7.1, 7.9, 3.5, 0.0, -2.5, 8.3, 8.0, 3.9, 1.1, -1… ## $ offensive_ranking \u0026lt;dbl\u0026gt; 0.0, 7.1, 1.4, 0.5, -2.7, 1.5, 0.0, 0.6, 3.2, -8… ## $ defensive_ranking \u0026lt;dbl\u0026gt; 7.1, 0.8, 2.2, -0.5, 0.2, 6.8, 8.0, 3.3, -2.1, -… ## $ playoffs \u0026lt;chr\u0026gt; \u0026quot;Playoffs\u0026quot;, \u0026quot;Playoffs\u0026quot;, \u0026quot;No Playoffs\u0026quot;, \u0026quot;No Playo… ## $ sb_winner \u0026lt;chr\u0026gt; \u0026quot;No Superbowl\u0026quot;, \u0026quot;No Superbowl\u0026quot;, \u0026quot;No Superbowl\u0026quot;, …  The question: to super bowl winners have harder schedules? I am curious if teams that win the super bowl have an easier or harder schedule than the other teams that make it to the playoffs. The strength_of_schedule variable provides a simple way to test this.\nThe only data manipulation we need to do is combine playoffs and sb_winner:\nstandings \u0026lt;- standings %\u0026gt;% mutate(final_position = if_else(sb_winner == \u0026quot;Won Superbowl\u0026quot;, sb_winner, playoffs))  Summarize data standing_summary \u0026lt;- standings %\u0026gt;% group_by(final_position) %\u0026gt;% summarize(less_zero = sum(strength_of_schedule \u0026lt; 0), total = n(), percent_less_zero = less_zero / total, median_strength = median(strength_of_schedule) ) standing_summary  ## # A tibble: 3 x 5 ## final_position less_zero total percent_less_zero median_strength ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 No Playoffs 170 398 0.427 0.2 ## 2 Playoffs 132 220 0.6 -0.5 ## 3 Won Superbowl 8 20 0.4 0.2  It looks like most teams that make it to the playoffs have an easier schedule (60% of them hace an average opponent rating less than zero) than those that win the super bowl (only 40% have average opponent rating less than zero). Teams that don\u0026rsquo;t make the playoffs also seem to have more difficult schedule.\nTest the difference in schedule strength The Kruskal-Wallis is suitable for multiple rank-based comparisons of groups:\nstandings %\u0026gt;% kruskal.test(strength_of_schedule ~ final_position, data = .)  ## ## Kruskal-Wallis rank sum test ## ## data: strength_of_schedule by final_position ## Kruskal-Wallis chi-squared = 27.31, df = 2, p-value = 1.174e-06  But sometimes, doing pairwise Wilcox tests can be more interpretable.\nstandings %\u0026gt;% filter(final_position != \u0026quot;Won Superbowl\u0026quot;) %\u0026gt;% wilcox.test(strength_of_schedule ~ final_position, data = .)  ## ## Wilcoxon rank sum test with continuity correction ## ## data: strength_of_schedule by final_position ## W = 54732, p-value = 2.549e-07 ## alternative hypothesis: true location shift is not equal to 0  standings %\u0026gt;% filter(final_position != \u0026quot;No Playoffs\u0026quot;) %\u0026gt;% wilcox.test(strength_of_schedule ~ final_position, data = .)  ## ## Wilcoxon rank sum test with continuity correction ## ## data: strength_of_schedule by final_position ## W = 1599, p-value = 0.04333 ## alternative hypothesis: true location shift is not equal to 0  Plot Manually specify some colors (taken from the fun wesanderson package)\nfinal_position_pal = c( \u0026quot;No Playoffs\u0026quot; = \u0026quot;#F2AD00\u0026quot;, \u0026quot;Playoffs\u0026quot; = \u0026quot;#00A08A\u0026quot;, \u0026quot;Won Superbowl\u0026quot; = \u0026quot;#FF0000\u0026quot; )  We will use the features of the great new ggtext package to color text in the subtitle of the plot, obviating the need for a figure legend.\nlibrary(ggtext) standings %\u0026gt;% ggplot(aes(x = final_position, y = strength_of_schedule, fill = final_position)) + geom_hline(yintercept = 0, lwd = 0.5, lty = 2, color = 'grey 50') + geom_boxplot() + #geom_jitter(width = 0.25) + geom_text(data = standing_summary, inherit.aes = FALSE, nudge_y = 0.22, mapping = aes(x = final_position, y = median_strength, label = median_strength)) + scale_fill_manual(values = final_position_pal, guide = 'none' ) + theme_minimal() + theme(axis.title = element_blank(), axis.text.x = element_blank(), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.title = element_textbox(), plot.subtitle = element_markdown(linewidth = 20) ) + labs(title = \u0026quot;**No Easy Road to Super Bowl Victory**\u0026quot;, subtitle = \u0026quot;Since 2000, NFL \u0026lt;b style='color:#F2AD00'\u0026gt;teams that miss the playoffs\u0026lt;/b\u0026gt; have a harder schedule (an average opponent rating greater than the dashed line at zero) than \u0026lt;b style='color:#00A08A'\u0026gt;teams that make it to the playoffs\u0026lt;/b\u0026gt;. A \u0026lt;b style='color:#FF0000'\u0026gt;Super Bowl winner's\u0026lt;/b\u0026gt; schedule is much tougher by comparison.\u0026quot; ) + ggsave(filename = 'featured.png', width = 7, height = 7)  Bonus: Color by Team Name How does it look to plot individual points for each team, coloring them accoring to their team colors (thanks to the teamcolors package)?\nteam_fill = teamcolors::league_pal('nfl', which = 2) team_color = teamcolors::league_pal('nfl', which = 1) ## Use just team name, not the home city (which changes for a couple teams) names(team_fill) \u0026lt;- str_remove(names(team_fill), \u0026quot;^.* \u0026quot;) names(team_color) \u0026lt;- str_remove(names(team_color), \u0026quot;^.* \u0026quot;) standings %\u0026gt;% ggplot(aes(x = final_position, y = strength_of_schedule, fill = team_name, color = team_name)) + geom_jitter(width = 0.4, pch = 23, size = 3) + scale_fill_manual(values = team_fill, guide = 'none' ) + scale_color_manual(values = team_color, guide = 'none' ) + theme_minimal() + theme(axis.title = element_blank(), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.title = element_textbox(), plot.subtitle = element_markdown(linewidth = 20) )  Ugly. Too many of the teams have colors that are close to one another.\nBonus: What is the bump in attendance the year after winning a superbowl? Making use of the lag() function, if you account for the fact that each team has a different \u0026ldquo;baseline\u0026rdquo; of attendance, it looks like the year after a superbowl win, attendence to away games may go up a bit. Home games and total attendance doesn\u0026rsquo;t really change though.\nattendance %\u0026gt;% select(team, team_name, year, total, home, away) %\u0026gt;% distinct() %\u0026gt;% group_by(team) %\u0026gt;% ## adjust attendance on a per-team basis to account for a difference baseline for each team mutate_at(vars(total, home, away), .funs = ~scale(., scale = FALSE) ) %\u0026gt;% ungroup() %\u0026gt;% full_join(standings) %\u0026gt;% group_by(team) %\u0026gt;% mutate_all(.funs = list(last_year = ~lag(.))) %\u0026gt;% filter(sb_winner_last_year == \u0026quot;Won Superbowl\u0026quot;) %\u0026gt;% filter(!is.na(sb_winner_last_year)) %\u0026gt;% ungroup() %\u0026gt;% arrange(team, year) %\u0026gt;% gather(attendance_the_year_after_winning_superbowl, value, total, total_last_year, home, home_last_year, away, away_last_year) %\u0026gt;% ggplot(aes(x = attendance_the_year_after_winning_superbowl, y = value)) + geom_boxplot() + ggpubr::stat_compare_means() + theme_minimal()  ## Joining, by = c(\u0026quot;team\u0026quot;, \u0026quot;team_name\u0026quot;, \u0026quot;year\u0026quot;) ## `mutate_all()` ignored the following grouping variables: ## Column `team` ## Use `mutate_at(df, vars(-group_cols()), myoperation)` to silence the message.  ","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580774400,"objectID":"4c9f34647c8762a229a04e3a13ab2a1d","permalink":"/post/2020-02-04-tt-football/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/post/2020-02-04-tt-football/","section":"post","summary":"Dip into the new ggttext package to get some rich text formatting in a plot\u0026rsquo;s title. Also: sports.\n","tags":["R","ggtext"],"title":"Tidy Tuesday: NFL Stadium Attendance","type":"post"},{"authors":["Tanner Koomar"],"categories":null,"content":"","date":1579219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579219200,"objectID":"2d0d10e6b344fcbbc2a8cf652a64f9c7","permalink":"/publication/2020-language-review-paper/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/publication/2020-language-review-paper/","section":"publication","summary":"This review examines the genetic and phenotypic overlaps of language impairments and neuropsychatric conditions.","tags":["autism","bipolar disorder","major depression","schizophrenia","adhd","language genetics"],"title":"Genetic Intersections of Language and Neuropsychiatric Conditions","type":"publication"},{"authors":["Tanner Koomar"],"categories":null,"content":"","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"4a20089737756f76b7f43c7ddbe31e0a","permalink":"/publication/2019-miccai-paper/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/publication/2019-miccai-paper/","section":"publication","summary":"Placed 2nd in an machine learning competition to predict fluid intellegence from structural brain MRI, sponsored by the Medical Image Computing and Computer Assisted Intervention conference.","tags":["MRI","Machie Learning","ABCD"],"title":"Ensemble Modeling of Neurocognitive Performance Using MRI-Derived Brain Structure Volumes","type":"publication"},{"authors":null,"categories":["tidytuesday"],"content":"Animated GIFs showing changes to funding of university research, broken down by speciality.\nIf you take a look at this table raw, it really is quite the mess. It is kind of hard to imagine data that is less tidy:\nlibrary(tidyverse) library(gganimate) library(showtext) ## for google fonts font_add_google(\u0026quot;Dosis\u0026quot;) font_add_google(\u0026quot;Comfortaa\u0026quot;) showtext_auto() download.file(\u0026quot;https://www.aaas.org/sites/default/files/2018-11/UniDisc1.xlsx\u0026quot;, \u0026quot;2019-05-08_UniDisc1.xlsx\u0026quot;) readxl::read_excel(\u0026quot;2019-05-08_UniDisc1.xlsx\u0026quot;)  ## # A tibble: 103 x 13 ## `University R\u0026amp;D… X__1 X__2 X__3 X__4 X__5 X__6 X__7 X__8 X__9 ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (budget authori… NA NA NA NA NA NA NA NA NA ## 2 \u0026lt;NA\u0026gt; 2007 2008 2009 2010 2011 2012 2013 2014 2015 ## 3 Computer Scienc… 1696. 1709. 1837 1864. 1943. 1996. 2228. 2040. 2051. ## 4 \u0026lt;NA\u0026gt; NA NA NA NA NA NA NA NA NA ## 5 Environmental S… NA NA NA NA NA NA NA NA NA ## 6 Atmospheric Sci… 529. 491. 478. 488. 537. 522. 531. 531. 600. ## 7 Earth Sciences 1080. 1104. 1170. 1233. 1274. 1280. 1253. 1226. 1164. ## 8 Oceanography 1176. 1220. 1237. 1163. 1170. 1123 1132. 1133. 1098. ## 9 Other Environme… 393. 434. 470 520. 535. 550. 524. 524. 533. ## 10 Total Environme… 3178. 3250. 3356. 3404 3516. 3475. 3441. 3414. 3394. ## # ... with 93 more rows, and 3 more variables: X__10 \u0026lt;dbl\u0026gt;, X__11 \u0026lt;dbl\u0026gt;, ## # X__12 \u0026lt;chr\u0026gt;  We have a couple header rows and then disciplines, followed by several sub-disciplines (or none). In Excel, this looks rather inteligible because of bold formating, etc. We have none of that here and have to get a litle creative to separate and assign disciplines to sub-disciplines.\nThe main pattern we have here is that every discipline row is preceded by an NA value. Combining dplyr::lag(), which gets the last value in a vector with zoo::locf(), which replaces NA\u0026rsquo;s with the most recent non-missing value.\ntot_dat \u0026lt;- readxl::read_excel(\u0026quot;2019-05-08_UniDisc1.xlsx\u0026quot;, skip = 2, ## header junk n_max = 47 ## 2nd table appears after here ) %\u0026gt;% mutate(discipline = case_when( is.na(lag(X__1)) ~ X__1)) %\u0026gt;% ## get the category columns select(X__1, discipline, everything()) %T\u0026gt;% ## these two lines are just for printing out the {print(head(.))} %\u0026gt;% ## intermediary results mutate(discipline = zoo::na.locf(discipline), funding_source = \u0026quot;total\u0026quot;) %T\u0026gt;% {print(head(.))} %\u0026gt;% filter(!is.na((X__1)), !is.na(`2007`), !str_detect(X__1, \u0026quot;Total\u0026quot;) ) %\u0026gt;% rename('sub_discipline' = 'X__1') %T\u0026gt;% {print(head(.))} %\u0026gt;% select(-`'07 - '17`) %\u0026gt;% gather(`2007`:`2017`, key = year, value = budget) %T\u0026gt;% {print(head(.))} %\u0026gt;% mutate(budget = budget*1e6)  ## # A tibble: 6 x 14 ## X__1 discipline `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Comp… Computer … 1696. 1709. 1837 1864. 1943. 1996. 2228. 2040. ## 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA NA NA NA NA NA NA NA ## 3 Envi… Environme… NA NA NA NA NA NA NA NA ## 4 Atmo… \u0026lt;NA\u0026gt; 529. 491. 478. 488. 537. 522. 531. 531. ## 5 Eart… \u0026lt;NA\u0026gt; 1080. 1104. 1170. 1233. 1274. 1280. 1253. 1226. ## 6 Ocea… \u0026lt;NA\u0026gt; 1176. 1220. 1237. 1163. 1170. 1123 1132. 1133. ## # ... with 4 more variables: `2015` \u0026lt;dbl\u0026gt;, `2016` \u0026lt;dbl\u0026gt;, `2017` \u0026lt;dbl\u0026gt;, ## # `'07 - '17` \u0026lt;dbl\u0026gt; ## # A tibble: 6 x 15 ## X__1 discipline `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Comp… Computer … 1696. 1709. 1837 1864. 1943. 1996. 2228. 2040. ## 2 \u0026lt;NA\u0026gt; Computer … NA NA NA NA NA NA NA NA ## 3 Envi… Environme… NA NA NA NA NA NA NA NA ## 4 Atmo… Environme… 529. 491. 478. 488. 537. 522. 531. 531. ## 5 Eart… Environme… 1080. 1104. 1170. 1233. 1274. 1280. 1253. 1226. ## 6 Ocea… Environme… 1176. 1220. 1237. 1163. 1170. 1123 1132. 1133. ## # ... with 5 more variables: `2015` \u0026lt;dbl\u0026gt;, `2016` \u0026lt;dbl\u0026gt;, `2017` \u0026lt;dbl\u0026gt;, ## # `'07 - '17` \u0026lt;dbl\u0026gt;, funding_source \u0026lt;chr\u0026gt; ## # A tibble: 6 x 15 ## sub_discipline discipline `2007` `2008` `2009` `2010` `2011` `2012` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Computer Scie… Computer … 1696. 1709. 1837 1864. 1943. 1996. ## 2 Atmospheric S… Environme… 529. 491. 478. 488. 537. 522. ## 3 Earth Sciences Environme… 1080. 1104. 1170. 1233. 1274. 1280. ## 4 Oceanography Environme… 1176. 1220. 1237. 1163. 1170. 1123 ## 5 Other Environ… Environme… 393. 434. 470 520. 535. 550. ## 6 Agricultural … Life Scie… 3453 3478. 3507. 3426 3476. 3614 ## # ... with 7 more variables: `2013` \u0026lt;dbl\u0026gt;, `2014` \u0026lt;dbl\u0026gt;, `2015` \u0026lt;dbl\u0026gt;, ## # `2016` \u0026lt;dbl\u0026gt;, `2017` \u0026lt;dbl\u0026gt;, `'07 - '17` \u0026lt;dbl\u0026gt;, funding_source \u0026lt;chr\u0026gt; ## # A tibble: 6 x 5 ## sub_discipline discipline funding_source year budget ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Computer Sciences Computer Sciences total 2007 1696. ## 2 Atmospheric Sciences Environmental Scie… total 2007 529. ## 3 Earth Sciences Environmental Scie… total 2007 1080. ## 4 Oceanography Environmental Scie… total 2007 1176. ## 5 Other Environmental Scie… Environmental Scie… total 2007 393. ## 6 Agricultural Sciences Life Sciences total 2007 3453  That looks much tidyr! Let\u0026rsquo;s do the same with the other table, a little bit more efficiently:\ntry( readxl::read_excel(\u0026quot;2019-05-08_UniDisc1.xlsx\u0026quot;, skip = 53, n_max = 47 ) %\u0026gt;% mutate(discipline = case_when( is.na(lag(X__1)) ~ X__1), discipline = zoo::na.locf(discipline), funding_source = \u0026quot;federal\u0026quot; ) %\u0026gt;% select(X__1, discipline, everything()) %\u0026gt;% filter(!is.na((X__1)), !is.na(`2007`), !str_detect(X__1, \u0026quot;Total\u0026quot;) ) %\u0026gt;% rename('sub_discipline' = 'X__1') %\u0026gt;% select(-`'07 - '17`) %\u0026gt;% gather(`2007`:`2017`, key = year, value = budget) %\u0026gt;% mutate(budget = budget*1e6) )  OH NO! There is an empty row after the header that wasn\u0026rsquo;t in the other table, and it is throwing us off. Let\u0026rsquo;s just remov that pesky row and pretend this never happened.\nfed_dat \u0026lt;- readxl::read_excel(\u0026quot;2019-05-08_UniDisc1.xlsx\u0026quot;, skip = 53, n_max = 47 ) %\u0026gt;% filter(!row_number() == 1) %\u0026gt;% mutate(discipline = case_when( is.na(lag(X__1)) ~ X__1), discipline = zoo::na.locf(discipline), funding_source = \u0026quot;federal\u0026quot; ) %\u0026gt;% select(X__1, discipline, everything()) %\u0026gt;% filter(!is.na((X__1)), !is.na(`2007`), !str_detect(X__1, \u0026quot;Total\u0026quot;) ) %\u0026gt;% rename('sub_discipline' = 'X__1') %\u0026gt;% select(-`'07 - '17`) %\u0026gt;% gather(`2007`:`2017`, key = year, value = budget) %\u0026gt;% mutate(budget = budget*1e6)  Finally, lets combine this all into one big tibble\ndat \u0026lt;- bind_rows(fed_dat, tot_dat) %\u0026gt;% spread(funding_source, budget) %\u0026gt;% mutate(other = total-federal) %\u0026gt;% gather(funding_source, budget, federal, other) %\u0026gt;% mutate(year = as.numeric(year), budget = ifelse(funding_source=='federal', budget, -budget)) %\u0026gt;% arrange(discipline) %\u0026gt;% mutate( sub_discipline = factor(sub_discipline, ordered = T, levels = unique(sub_discipline)) )  A new theme theme_fnd \u0026lt;- function(base_size = 11, base_family = \u0026quot;Comfortaa\u0026quot;, base_line_size = base_size/22, base_rect_size = base_size/22) { theme_minimal(base_size = base_size, base_family = base_family, base_line_size = base_line_size, base_rect_size = base_rect_size) %+replace% theme(plot.background = element_rect(fill = '#e5d3ac'), panel.background = element_rect(fill = '#efe5c8', color = NA), panel.grid = element_line(color = '#640c14', size = 0.15)) + theme(plot.margin = margin(20,20,5,25), text = element_text(colour = '#640c14'), axis.text = element_text(colour = '#640c14'), axis.text.y = element_text(margin = margin(2,2,2,20)), axis.text.x.top = element_text(size = 11), axis.ticks.x.top = element_blank(), plot.title = element_text(family = \u0026quot;Dosis\u0026quot;, size = 18, face = 'bold'), plot.subtitle = element_text(family = \u0026quot;Dosis\u0026quot;, hjust = 1) ) }  plot_1 \u0026lt;- dat %\u0026gt;% filter( discipline != \u0026quot;Life Sciences\u0026quot;) %\u0026gt;% ggplot(aes(y = sub_discipline, color = funding_source)) + geom_segment(aes(x = 0, xend = budget, yend = sub_discipline, color = funding_source), size = 2.5) + geom_point(aes(x = budget, y = sub_discipline, fill = discipline), pch = 21, size = 8 , stroke = 0) + scale_fill_manual(values = c('Physical Sciences' = '#fcaf43', 'Computer Sciences' = '#e61e28', 'Environmental Sciences' = '#71a234', 'Social Sciences' = '#207e76', 'Psychology' = '#8dc642', 'Mathematical Sciences' = '#26bdb2', 'Engineering' = '#f26630', 'Other Sciences' = '#c21a24'), guide = guide_legend(title.position = 'top', title.hjust = 0.5, ncol = 2 )) + scale_color_manual(values = c('#5e2b15', '#9d4823'), guide = 'none') + scale_x_continuous(breaks = c(-1.5e9,0,2e9), minor_breaks = c(-.5e9,-1e9,.5e9,1e9,1.5e9), limits = c(-1.5e9,2e9), labels = c(\u0026quot;$1.5 Mil\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;$2 Mil\u0026quot;), position ='bottom', sec.axis = sec_axis(~., breaks = c(-0.75e9,1e9), labels = c(\u0026quot;all other funding\u0026quot;, \u0026quot;federal funding\u0026quot;) ) ) + labs(y = element_blank(), x = element_blank(), title = \u0026quot;funding for university research in {closest_state}\u0026quot;, subtitle = \u0026quot;(excluding life sciences)\u0026quot;, caption = \u0026quot;A #TidyTuesday Adventure by @TannerKoomar\\nsource: American Association for the Advancement of Science\u0026quot;) + theme_fnd() + theme(legend.position = 'bottom', legend.text = element_text(margin = margin(5,40,5,0)), panel.grid.major.y = element_blank()) + transition_states( year, transition_length = 2, state_length = 1 ) anim_save(animation = plot_1, filename = \u0026quot;final_plot_1.gif\u0026quot;, width = 700, height = 700)  plot_2 \u0026lt;- dat %\u0026gt;% filter( discipline == \u0026quot;Life Sciences\u0026quot;) %\u0026gt;% ggplot(aes(y = sub_discipline, color = funding_source)) + geom_segment(aes(x = 0, xend = budget, yend = sub_discipline, color = funding_source), size = 3) + geom_point(aes(x = budget, y = sub_discipline), pch = 21, size = 7 , stroke = 0, fill = '#71a234') + scale_color_manual(values = c('#5e2b15', '#9d4823'), guide = 'none') + scale_x_continuous(breaks = c(-12e9,0,16e9), minor_breaks = c(-4e9,-8e9,4e9,8e9), limits = c(-12e9,16e9), labels = c(\u0026quot;$12 Mil\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;$16 Mil\u0026quot;), position ='bottom', sec.axis = sec_axis(~., breaks = c(-6e9,8e9), labels = c(\u0026quot;all other funding\u0026quot;, \u0026quot;federal funding\u0026quot;) ) ) + labs(y = element_blank(), x = element_blank(), title = \u0026quot;funding for university research in {closest_state}\u0026quot;, caption = \u0026quot;A #TidyTuesday Adventure by @TannerKoomar\\nsource: American Association for the Advancement of Science\u0026quot;) + theme_fnd() + theme(legend.position = 'bottom', panel.grid.major.y = element_blank()) + transition_states( year, transition_length = 2, state_length = 1 ) anim_save(animation = plot_2, filename = \u0026quot;final_plot_2.gif\u0026quot;, width = 700, height = 350)  ","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"35e5ac8374ce37090c63b8b1c041866e","permalink":"/post/2019-05-08-tt-uni-funding/","publishdate":"2019-05-08T00:00:00Z","relpermalink":"/post/2019-05-08-tt-uni-funding/","section":"post","summary":"Animated GIFs showing changes to funding of university research, broken down by speciality.\n","tags":["R","gif"],"title":"Tidy Tuesday: University Research Funding","type":"post"},{"authors":null,"categories":["tidytuesday"],"content":"A look at housing price trends over the last century with a compound infographic.\n  format data   A new theme   violin plot   statebin maps   line plots   map key   texts   title   narrative   footer     Asemble   The final Plot   library(tidyverse) library(statebins) library(cowplot) library(magick) library(showtext) ## for google fonts font_add_google(\u0026quot;Staatliches\u0026quot;) font_add_google(\u0026quot;Oswald\u0026quot;) font_add_google(\u0026quot;Montserrat\u0026quot;, regular.wt = 300, bold.wt = 500) showtext_auto() ## read in data hpi_dat \u0026lt;- read_csv(\u0026quot;https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-02-05/state_hpi.csvhttps://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-02-05/state_hpi.csv\u0026quot;)  ## Parsed with column specification: ## cols( ## year = col_integer(), ## month = col_integer(), ## state = col_character(), ## price_index = col_double(), ## us_avg = col_double() ## )  format data hpi_dat \u0026lt;- hpi_dat %\u0026gt;% mutate(years = cut(year, breaks = c(1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2018), labels = c(1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015), include.lowest = TRUE ) ) %\u0026gt;% group_by(years) %\u0026gt;% mutate( price_ratio = price_index/us_avg, price_sd = sd(price_index/us_avg) ) %\u0026gt;% ungroup() %\u0026gt;% left_join(tibble(state = state.abb, region = as.character(state.division))) %\u0026gt;% mutate(region = replace_na(region, \u0026quot;Washington DC\u0026quot;))  ## Joining, by = \u0026quot;state\u0026quot;  A new theme theme_house\u0026lt;- function(base_size = 11, base_family = \u0026quot;Oswald\u0026quot;, base_line_size = base_size/22, base_rect_size = base_size/22) { theme_minimal(base_size = base_size, base_family = base_family, base_line_size = base_line_size, base_rect_size = base_rect_size) %+replace% theme(axis.title = element_text(face = 'bold', hjust = 0, size = 14), strip.text = element_text(hjust = 1)) }  violin plot violin \u0026lt;- hpi_dat %\u0026gt;% ggplot(aes(x = years, y = (price_ratio), fill = price_sd)) + geom_hline(yintercept = 1 , lty = 2) + geom_violin(color = NA, alpha = 0.85) + scale_fill_gradientn(colors = c(\u0026quot;#F98400\u0026quot;,\u0026quot;#ECCBAE\u0026quot;), name = 'standard deviation of home prices') + labs(x = \u0026quot;5 year period\u0026quot;, y = \u0026quot;HPI relative to national average\u0026quot; ) + guides(fill = guide_colorbar(title.position = \u0026quot;top\u0026quot;, title.hjust = 0.5, label.position = 'bottom', barwidth = 15, barheight = .5) ) + scale_y_continuous(trans = 'log2', breaks = c(1/2, 1, 2), labels = c(\u0026quot;1/2x\u0026quot;, \u0026quot;1x\u0026quot;, \u0026quot;2x\u0026quot;), limits = c(1/2,2)) + #scale_fill_viridis_c(direction = -1, option = \u0026quot;E\u0026quot;) + theme_house() + theme(legend.direction = 'horizontal', legend.justification = c(.5, 0), legend.position = c(0.5, 0.78) ) violin  ## Warning: Removed 1 rows containing non-finite values (stat_ydensity).  statebin maps bin_maps \u0026lt;- hpi_dat %\u0026gt;% filter(years %in% c(1975, 1995, 2015)) %\u0026gt;% ggplot(aes(state = state, fill = cut(log2(price_ratio), breaks = c(-Inf, -.625, -.375, -.125, .125, .375, .625, Inf), include.lowest = TRUE, labels = c(\u0026quot;-1\u0026quot;, \u0026quot;-1/2\u0026quot;, \u0026quot;-1/4\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;1/4\u0026quot;, \u0026quot;1/2\u0026quot;, \u0026quot;1\u0026quot;) ) ) ) + geom_statebins(border_size = 0, dark_lbl = NA, light_lbl = NA) + facet_wrap(~years, nrow = 3) + coord_equal() + guides(fill = guide_legend(title.position = 'top', title.hjust = 0.5, label.position = 'bottom', direction = 'horizontal', nrow = 1, keyheight = .75, keywidth = c(2, 1, 1, 2, 1, 1, 2) ) ) + scale_fill_manual(values = colorRampPalette(c(\u0026quot;#FF0000\u0026quot;, \u0026quot;#fbf4ee\u0026quot;, \u0026quot;#00A08A\u0026quot;))(7), name = \u0026quot;HPI relative to national average\u0026quot;, labels = c(\u0026quot;1/2x\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;1x\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot; 2x\u0026quot;)) + ggtitle(\u0026quot;Home Prices in Flyover\\nCountry Can't Keep Up\u0026quot;) + theme_void() + theme(plot.title=element_text(family = \u0026quot;Oswald\u0026quot;, size=20, hjust=1, lineheight = 0.85), plot.margin = margin(0,10,0,10), strip.text = element_text(family = \u0026quot;Oswald\u0026quot;, size = 24, hjust = 0), legend.position = 'top', legend.justification = 0.5, legend.direction = 'horizontal', legend.text = element_text(family = \u0026quot;Oswald\u0026quot;) ) bin_maps  line plots lines \u0026lt;- hpi_dat %\u0026gt;% group_by(year, region) %\u0026gt;% ggplot(aes(x = year, y = price_ratio, color = region, group = region)) + geom_hline(yintercept = 1, lty = 2, lwd = 0.25) + scale_y_continuous(trans = 'log2', breaks = c(1/2, 1, 2), labels = c(\u0026quot;1/2x\u0026quot;, \u0026quot;1x\u0026quot;, \u0026quot;2x\u0026quot;), limits = c(1/2,2)) + labs(y = \u0026quot;HPI relative to national average\u0026quot;) + geom_line(alpha = 0.85, size = 4, show.legend = FALSE) + facet_wrap(~region, ncol = 2) + scale_color_manual(values = c(\u0026quot;#FF0000\u0026quot;, \u0026quot;#00A08A\u0026quot;, \u0026quot;#F2AD00\u0026quot;, \u0026quot;#F98400\u0026quot;, \u0026quot;#5bd679\u0026quot;, \u0026quot;#ECCBAE\u0026quot;, \u0026quot;#046C9A\u0026quot;, \u0026quot;#000000\u0026quot;, \u0026quot;#ABDDDE\u0026quot;, \u0026quot;#D69C4E\u0026quot; )) + theme_house() + theme(axis.title = element_text(size = 22)) lines  ## Warning: Removed 1 rows containing missing values (geom_path).  map key bin_map_key \u0026lt;- hpi_dat %\u0026gt;% left_join(tibble(state = state.abb, region = state.division)) %\u0026gt;% ggplot(aes(state = state, fill = region)) + geom_statebins(border_size = 0, family = \u0026quot;Montserrat\u0026quot; ) + scale_fill_manual(values = c(\u0026quot;#FF0000\u0026quot;, \u0026quot;#00A08A\u0026quot;, \u0026quot;#F2AD00\u0026quot;, \u0026quot;#F98400\u0026quot;, \u0026quot;#5bd679\u0026quot;, \u0026quot;#ECCBAE\u0026quot;, \u0026quot;#046C9A\u0026quot;, \u0026quot;#000000\u0026quot;, \u0026quot;#ABDDDE\u0026quot;, \u0026quot;#D69C4E\u0026quot;)) + coord_equal() + ggtitle(\u0026quot;Regions of the United States\u0026quot;) + theme_void() + theme(plot.title=element_text(family = \u0026quot;Oswald\u0026quot;, size=20, hjust=0), plot.margin = margin(10,10,10,10), legend.position = 'none' )  ## Joining, by = c(\u0026quot;state\u0026quot;, \u0026quot;region\u0026quot;) ## Warning: Column `region` joining character vector and factor, coercing into ## character vector  bin_map_key  texts title header \u0026lt;- ggplot() + annotate(geom = \u0026quot;text\u0026quot;, y = -0.1, x = 0.5, size = 10, family = \u0026quot;Staatliches\u0026quot;, color = 'white', alpha = 1, lineheight = 0.7, hjust = 1, label=\u0026quot;HOW THE HOUSING\\nPRICE INDEX\u0026quot; ) + annotate(geom = \u0026quot;text\u0026quot;, y = -0.1, x = .79, size = 12, family = \u0026quot;Staatliches\u0026quot;, color = 'white', alpha = 1, lineheight = 0.9, hjust = 0, label=\u0026quot;FLIPPED\u0026quot; ) + ylim(-2,2) + xlim(-2,2) + theme_void() + theme( plot.background = element_rect(color = \u0026quot;white\u0026quot;, fill = \u0026quot;transparent\u0026quot;, size = 5) ) ggdraw() + draw_image(\u0026quot;house.png\u0026quot;) + draw_plot(header)  narrative narr \u0026lt;- ggplot() + annotate(geom = \u0026quot;text\u0026quot;, y = 0, x= -2, size = 5.5, family = \u0026quot;Montserrat\u0026quot;, color = 'black', alpha = 0.75, lineheight = 0.9, hjust = 0, label=str_wrap(\u0026quot;In the 1970's, many individual states had average home prices (HPI) above the national average. The spread of house prices narrowed, until the mid 1990's. After this inflection point, house prices again grew more extreme. Less populous states drive this trend, with home prices in these regions going from above average to below it.\u0026quot;, 30) ) + ylim(-2,2) + xlim(-2,2) + theme_void() + theme( plot.background = element_rect(color = \u0026quot;white\u0026quot;, fill = '#e5e5ff', size = 5) ) narr  footer foot \u0026lt;- ggplot() + annotate(geom = \u0026quot;text\u0026quot;, y = 0, x= 0, size = 3, family = \u0026quot;Montserrat\u0026quot;, color = 'black', alpha = 0.5, hjust = 0.5, label= \u0026quot;◊ A #TidyTuesday adventure by @TannerKoomar ◊ Data from Freddie Mac ◊ Photo from pexels.com ◊\u0026quot; ) + ylim(-2,2) + xlim(-2,2) + theme_void() + theme( plot.background = element_rect(color = \u0026quot;white\u0026quot;, fill = '#e5e5ff', size = 3) ) foot  Asemble let\u0026rsquo;s try cowplot this time\nfinal_plot \u0026lt;- plot_grid( (ggdraw() + draw_image(\u0026quot;house.png\u0026quot;) + draw_plot(header)), plot_grid(violin, narr, rel_widths = c(5,3) ), plot_grid( plot_grid(bin_maps + theme(plot.background = element_rect(fill = alpha('#e5e5ff', 0.25), color = 'white', size = 2) ), bin_map_key, ncol = 1, rel_heights = c(3,1) ), lines, rel_widths = c(3, 5) ), foot, ncol = 1, rel_heights = c(1, 2, 6, 0.15) ) png(\u0026quot;final_plot.png\u0026quot;, width = 1000, height = 2000, res = 144, bg = \u0026quot;white\u0026quot;) final_plot dev.off()  ## png ## 2  The final Plot ","date":1555804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555804800,"objectID":"59c4d13ada28f84091e48266c2019218","permalink":"/post/2019-04-21-tt-housing-prices/","publishdate":"2019-04-21T00:00:00Z","relpermalink":"/post/2019-04-21-tt-housing-prices/","section":"post","summary":"A look at housing price trends over the last century with a compound infographic.\n","tags":["R","map","gridExtra"],"title":"Tidy Tuesday: How The Housing Price Index Flipped","type":"post"},{"authors":null,"categories":["tidytuesday"],"content":"A simple animated gif map.\n  MAKE A MAP   Animate map     The final Plot   library(tidyverse)  ## ── Attaching packages ─────────────────── ## ✔ ggplot2 3.1.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.7 ## ✔ tidyr 0.8.2 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()  require(transformr)  ## Loading required package: transformr  library(urbnmapr) library(gridExtra)  ## ## Attaching package: 'gridExtra' ## The following object is masked from 'package:dplyr': ## ## combine  library(gganimate) library(patchwork) source('https://raw.githubusercontent.com/tkoomar/ggplot2_themes/master/theme_black.R')  ## Loading required package: showtext ## Loading required package: sysfonts ## Loading required package: showtextdb  milk_state \u0026lt;- read_csv(\u0026quot;../data/2019/2019-01-29/state_milk_production.csv\u0026quot;)  ## Parsed with column specification: ## cols( ## region = col_character(), ## state = col_character(), ## year = col_integer(), ## milk_produced = col_double() ## ) ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 1) ## Warning: 50 parsing failures. ## row # A tibble: 5 x 5 col row col expected actual file expected \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; actual 1 1501 year no trailing chara… e3 '../data/2019/2019-01-29/state_mil… file 2 1502 year no trailing chara… e3 '../data/2019/2019-01-29/state_mil… row 3 1503 year no trailing chara… e3 '../data/2019/2019-01-29/state_mil… col 4 1504 year no trailing chara… e3 '../data/2019/2019-01-29/state_mil… expected 5 1505 year no trailing chara… e3 '../data/2019/2019-01-29/state_mil… ## ... ................. ... ........................................................................... ........ ........................................................................... ...... ........................................................................... .... ........................................................................... ... ........................................................................... ... ........................................................................... ........ ........................................................................... ## See problems(...) for more details.  milk_state \u0026lt;- milk_state %\u0026gt;% rename('state_name' = 'state') data('states') milk_map \u0026lt;- left_join(milk_state, states)  ## Joining, by = \u0026quot;state_name\u0026quot;  MAKE A MAP map_plot \u0026lt;- milk_map %\u0026gt;% group_by(year) %\u0026gt;% mutate(milk_total = sum(milk_produced), milk_scaled = milk_produced/milk_total ) %\u0026gt;% ggplot(aes(x = long, y = lat, fill = milk_scaled, group = group)) + geom_polygon(size = 0.25, color = 'black') + coord_map(projection = \u0026quot;albers\u0026quot;, lat0 = 39, lat1 = 45) + scale_fill_viridis_c(option = \u0026quot;inferno\u0026quot;)+ #scale_fill_distiller(palette = 'PuBuGn', direction = 1) + theme_black() + theme(legend.position = 'none', axis.text = element_blank(), axis.ticks = element_blank(), axis.line = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) + ggtitle(\u0026quot;portion of milk production by year\u0026quot;, \u0026quot;{closest_state}\u0026quot;)  Animate map map_anim \u0026lt;- map_plot + transition_states(states = year, transition_length = 5, state_length = 5, wrap = TRUE) anim_save(animation = map_anim, filename = \u0026quot;final_plot.gif\u0026quot;, width = 640 )  ## Warning in lapply(row_vars$states, as.integer): NAs introduced by coercion ## Warning in f(..., self = self): NAs introduced by coercion  The final Plot ","date":1551484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551484800,"objectID":"9623b76533d67c6757a05e6994ed13bf","permalink":"/post/2019-03-02-tt-milk-gif/","publishdate":"2019-03-02T00:00:00Z","relpermalink":"/post/2019-03-02-tt-milk-gif/","section":"post","summary":"A simple animated gif map.\n","tags":["R","gif"],"title":"Tidy Tuesday: Milk Production","type":"post"},{"authors":null,"categories":["tidytuesday"],"content":"  make some new columns   Set lables of variables of interest   Main plot   Morning   afternoon   sidebar   footer     The final Plot   library(tidyverse)  ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.7 ## ✔ tidyr 0.8.2 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()  library(lubridate)  ## ## Attaching package: 'lubridate' ## The following object is masked from 'package:base': ## ## date  library(gridExtra)  ## ## Attaching package: 'gridExtra' ## The following object is masked from 'package:dplyr': ## ## combine  source(\u0026quot;https://raw.githubusercontent.com/tkoomar/ggplot2_themes/master/theme_cc.R\u0026quot;)  ## Loading required package: showtext ## Loading required package: sysfonts ## Loading required package: showtextdb  tidytuesday_tweets \u0026lt;- readRDS(\u0026quot;../data/2019/2019-01-01/tidytuesday_tweets.rds\u0026quot;) rstats_tweets \u0026lt;- readRDS(\u0026quot;../data/2019/2019-01-01/rstats_tweets.rds\u0026quot;) ## helper function for formatting correlation output slope_conf \u0026lt;- function(x, y){ res \u0026lt;- lm(y ~ x) conf \u0026lt;- confint(res, \u0026quot;x\u0026quot;, 0.95) paste0(\u0026quot;95% confidence interval: \u0026quot;, round(conf[1], 3), \u0026quot;-\u0026quot;, round(conf[2], 3)) }  make some new columns We want to get the day of the week, and claculate the number of hastags per tweet.\nrstats_tweets \u0026lt;-rstats_tweets %\u0026gt;% mutate(w_day = wday(created_at, label = TRUE), hour = hour(created_at)) %\u0026gt;% rowwise() %\u0026gt;% mutate(n_hash = length(hashtags))  Set lables of variables of interest dat \u0026lt;- rstats_tweets %\u0026gt;% mutate(\u0026quot;retweet per hastag enrichment\u0026quot; = log2(retweet_count/n_hash), \u0026quot;number of hastags\u0026quot; = n_hash, \u0026quot;number of retweets\u0026quot; = retweet_count ) %\u0026gt;% gather(key, value, \u0026quot;retweet per hastag enrichment\u0026quot;, \u0026quot;number of hastags\u0026quot;, \u0026quot;number of retweets\u0026quot;)  Main plot morning_col \u0026lt;- alpha(\u0026quot;#ffd92f\u0026quot;, 0.35) noon_col \u0026lt;- alpha(\u0026quot;#e5c494\u0026quot;, 0.4) p_week \u0026lt;- dat %\u0026gt;% ggplot(aes(x = hour, y = value, color = key)) + annotate('rect', size = 0, xmin = 5, xmax = 9, ymin = -1, ymax = 5, fill = morning_col) + annotate('rect', size = 0, xmin = 12, xmax = 20, ymin = -1, ymax = 5, fill = noon_col) + stat_smooth(alpha = 0.1, size = 1.5) + geom_hline(yintercept = 0, lty = 3) + scale_color_brewer(palette = \u0026quot;Set2\u0026quot;) + facet_wrap(~ w_day, nrow = 1) + coord_cartesian(ylim = c(-1,5)) + ylab(\u0026quot;\u0026quot;) + theme_cc(base_size = 12) + theme(legend.position = \u0026quot;bottom\u0026quot;, plot.title = element_text(size = 26)) + ggtitle(\u0026quot;#rstats aren't early morning grinders\u0026quot;, \u0026quot;the average number of retweets and hashtags per #rstats tweet, by day and time\u0026quot;) p_week  ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \u0026quot;cs\u0026quot;)' ## Warning: Removed 243274 rows containing non-finite values (stat_smooth).  Morning p_morning \u0026lt;- dat %\u0026gt;% filter(hour \u0026gt;= 5 \u0026amp; hour \u0026lt;= 9 ) %\u0026gt;% filter(retweet_count \u0026lt; 500) %\u0026gt;% ## there are a couple big outliers here { ggplot(., aes(x = as.factor(n_hash), y = retweet_count)) + #geom_jitter(color = alpha(noon_col, 0.05)) + geom_boxplot(color = \u0026quot;#b3b3b3\u0026quot;, fill = morning_col, outlier.colour = alpha(\u0026quot;#b3b3b3\u0026quot;, 0.02)) + stat_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;#e78ac3\u0026quot;, size = 2, aes(x = (n_hash), y = retweet_count)) + coord_cartesian(xlim = c(0, 19.75 ), ylim = c(0, 19.75)) + ggtitle(slope_conf(.$n_hash, .$retweet_count)) + theme_cc(base_size = 12) + xlab(\u0026quot;number of hashtags\u0026quot;) + ylab(\u0026quot;number of retweets\u0026quot;) } p_morning  ### sidebar\nmorning_sidebar \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 2, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Poppins\u0026quot;, size = 5, color = \u0026quot;grey50\u0026quot;, label = str_wrap(string = \u0026quot;In general, more hashtags = more retweets. Early morning tweets usually contain more hashtags than tweets later in the day. Fewer people are on twitter at this time, so the apparent effectiveness of hashtags at provoking retweets is reduced. This begs the question: Why do people use more hashtags in the morning?\u0026quot;, width = 18) ) + theme_void() + theme( plot.background = element_rect( fill = morning_col,color = \u0026quot;white\u0026quot;, size = 2 ) ) morning_sidebar  afternoon p_noon \u0026lt;- dat %\u0026gt;% filter(hour \u0026gt;= 12 \u0026amp; hour \u0026lt;= 20 ) %\u0026gt;% filter(retweet_count \u0026lt; 500) %\u0026gt;% ## there are a couple big outliers here { ggplot(., aes(x = as.factor(n_hash), y = retweet_count)) + #geom_jitter(color = alpha(noon_col, 0.05)) + geom_boxplot(color = \u0026quot;#b3b3b3\u0026quot;, fill = noon_col, outlier.colour = alpha(\u0026quot;#b3b3b3\u0026quot;, 0.02)) + stat_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;#e78ac3\u0026quot;, size = 2, aes(x = (n_hash), y = retweet_count)) + coord_cartesian(xlim = c(0, 19.75 ), ylim = c(0, 19.75)) + ggtitle(slope_conf(.$n_hash, .$retweet_count)) + theme_cc(base_size = 12) + xlab(\u0026quot;number of hashtags\u0026quot;) + ylab(\u0026quot;number of retweets\u0026quot;) } p_noon  sidebar noon_sidebar \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 2, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Poppins\u0026quot;, size = 5, color = \u0026quot;grey50\u0026quot;, label = str_wrap(string = \u0026quot;Hashtags used in the mid-afternoon, when most people are on twitter, are almost twice as effective at provoking retweets! Also, while most days see a single pronounced increase in tweets, Sunday afternoon sees two distinct bumps in #rstats tweets.\u0026quot;, width = 18) ) + theme_void() + theme( plot.background = element_rect( fill = noon_col, color = \u0026quot;white\u0026quot;, size = 2 ), plot.margin = unit(c(.01,.01,.01,.01), 'npc') ) noon_sidebar  footer footer \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 2, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Poppins\u0026quot;, size = 2.5, color = \u0026quot;grey50\u0026quot;, label = paste0(\u0026quot;A #TidyTuesday adventure\\n\u0026quot;, \u0026quot;Data from rtweet.info\\n\u0026quot;, \u0026quot;Analysis @Tanner Koomar\\n\u0026quot;, \u0026quot;Design @Tanner Koomar\\n\u0026quot;, \u0026quot;https://github.com/tkoomar/\\ntidytuesday/blob/master/\\nwork/2019-01-01.md\u0026quot;) ) + theme_void() + theme( plot.background = element_rect( fill = alpha(\u0026quot;#b3b3b3\u0026quot;, 0.5), color = \u0026quot;white\u0026quot;, size = 2 ), plot.margin = unit(c(.01,.01,.01,.01), 'npc') ) footer  Assemble! #### legend\nplot_leg \u0026lt;- cowplot::get_legend(p_week + theme(legend.title = element_blank(), legend.justification = 'right')) plot_leg  ## TableGrob (5 x 5) \u0026quot;guide-box\u0026quot;: 2 grobs ## z cells name ## 99_e27293391d92b7a74a1c708851c8c43e 1 (3-3,3-3) guides ## 0 (2-4,2-4) legend.box.background ## grob ## 99_e27293391d92b7a74a1c708851c8c43e gtable[layout] ## zeroGrob[NULL]  png(\u0026quot;featured.png\u0026quot;, width = 1200, height = 900, res = 144, bg = \u0026quot;white\u0026quot;) grid.arrange( p_week + theme(legend.position = 'none'), morning_sidebar, p_morning + theme(axis.text.x = element_blank()), p_noon + theme(axis.text.x = element_blank()), noon_sidebar, footer, plot_leg, heights = c(.55,.15, .25,.25,.15), widths = c(.5, 1, 1, .5), layout_matrix = rbind(c(1,1,1,1), c(2,7,7,5), c(2,3,4,5), c(2,3,4,5), c(2,3,4,6) ) )  ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \u0026quot;cs\u0026quot;)' ## Warning: Removed 243274 rows containing non-finite values (stat_smooth).  dev.off()  ## png ## 2  The final Plot ","date":1550016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550016000,"objectID":"3174b91fcf4fb35d63ed4cbf380bd199","permalink":"/post/2019-02-13-tt-rstats-tweets/","publishdate":"2019-02-13T00:00:00Z","relpermalink":"/post/2019-02-13-tt-rstats-tweets/","section":"post","summary":"","tags":["R","twitter"],"title":"Tidy Tuesday: R Tweets","type":"post"},{"authors":null,"categories":["tidytuesday"],"content":"Some light web-scraping and natual language processing reveals how different parts of speech may hint at an author\u0026rsquo;s intent (and impact).\n  Tidy the text   infinitives   all verbs     some plots   scatter plots   tables     break vs build   topics / tags plot   time to (ha)rvest some posts!   sentiment plot   violin plot   legend     text boxes   header   definition sidebar   top sidebar   bottom sidebar   footer     stitch it together   The final Plot   Tidy the text following along with this: https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html\ninfinitives dat_infin \u0026lt;- dat %\u0026gt;% unnest_tokens(infin, title, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% filter(str_detect(pattern = \u0026quot;to \u0026quot;, string = infin)) %\u0026gt;% mutate(word = str_remove(infin, \u0026quot;to \u0026quot;)) %\u0026gt;% left_join(parts_of_speech) %\u0026gt;% mutate(word_stem = wordStem(word, \u0026quot;english\u0026quot;)) %\u0026gt;% filter(str_detect(pos, regex(\u0026quot;Verb\u0026quot;, ignore_case = F)), !duplicated(x1)) %\u0026gt;% group_by(word_stem) %\u0026gt;% mutate(n = n()) %\u0026gt;% group_by(word_stem, n) %\u0026gt;% summarize(mean_claps = mean(claps), sum_claps = sum(claps), infin = first(infin) )  ## Joining, by = \u0026quot;word\u0026quot;  all verbs dat_verb \u0026lt;- dat %\u0026gt;% unnest_tokens(word, title) %\u0026gt;% left_join(parts_of_speech) %\u0026gt;% filter(str_detect(pos, regex(\u0026quot;Verb\u0026quot;, ignore_case = F)), !duplicated(x1)) %\u0026gt;% mutate(word_stem = wordStem(word, \u0026quot;english\u0026quot;)) %\u0026gt;% group_by(word_stem) %\u0026gt;% mutate(n = n()) %\u0026gt;% group_by(word_stem, n) %\u0026gt;% summarize(mean_claps = mean(claps), sum_claps = sum(claps), word = first(word) )  ## Joining, by = \u0026quot;word\u0026quot;  some plots scatter plots plot_point_infin \u0026lt;- dat_infin %\u0026gt;% filter(n \u0026gt; 5) %\u0026gt;% ggplot(aes(x = n, y = mean_claps, color = case_when( mean_claps \u0026gt; 500 ~ \u0026quot;high impact infinitives\u0026quot;, n \u0026gt; 100 ~ \u0026quot;overused infinitives\u0026quot;, TRUE ~ \u0026quot;low impact infinitives\u0026quot; ), label = case_when( mean_claps \u0026gt; 500 | n \u0026gt; 100 ~ infin, mean_claps \u0026gt; 200 \u0026amp; n \u0026gt; 50 ~ infin, TRUE ~ \u0026quot;\u0026quot;) ) ) + scale_colour_manual(values = c(colors_merb[3], colors_merb[1], colors_merb[2]), guide = guide_legend(title = NULL, override.aes = list(size = 4)) )+ geom_point() + geom_vline(xintercept = 100, lwd = 2, lty = 2, alpha = 0.3, color = \u0026quot;grey90\u0026quot;) + geom_hline(yintercept = 500, lwd = 2, lty = 2, alpha = 0.3, color = \u0026quot;grey90\u0026quot;) + geom_text_repel(force = 15, show.legend = FALSE) + theme_merb() + xlab(\u0026quot;number of times used\u0026quot;) + ylab(\u0026quot;mean number of claps\u0026quot;) + theme( legend.text = element_text(size = 12), legend.box.just = \u0026quot;right\u0026quot; ) plot_point_infin  dat_verb %\u0026gt;% filter(n \u0026gt; 5) %\u0026gt;% ggplot(aes(x = n, y = mean_claps, color = case_when( mean_claps \u0026gt; 500 ~ \u0026quot;high_impact\u0026quot;, n \u0026gt; 100 ~ \u0026quot;overused\u0026quot;, TRUE ~ \u0026quot;misc\u0026quot; ), label = case_when( mean_claps \u0026gt; 500 | n \u0026gt; 100 ~ word, mean_claps \u0026gt; 200 \u0026amp; n \u0026gt; 50 ~ word, TRUE ~ \u0026quot;\u0026quot;))) + scale_colour_manual(values = c(colors_merb[3], colors_merb[1], colors_merb[2])) + geom_point() + geom_vline(xintercept = 100, lwd = 2, lty = 2, alpha = 0.3, color = \u0026quot;grey90\u0026quot;) + geom_hline(yintercept = 500, lwd = 2, lty = 2, alpha = 0.3, color = \u0026quot;grey90\u0026quot;) + geom_text_repel(force = 15) + theme_merb() + xlab(\u0026quot;number of times used\u0026quot;) + ylab(\u0026quot;mean number of claps\u0026quot;) + theme( legend.position = \u0026quot;none\u0026quot; )  tables gtable_theme \u0026lt;- ttheme_minimal( core = list(fontfamily = \u0026quot;Share Tech Mono\u0026quot;, bg_params = list(fill = c(\u0026quot;#414C3B\u0026quot;, \u0026quot;#191919\u0026quot;)), fg_params = list( col = matrix(c(\u0026quot;white\u0026quot;, colors_merb[2], colors_merb[3]), nrow = 10, ncol = 3, byrow = T), fontsize = 12 ) ), colhead = list( bg_params = list(fill = \u0026quot;#222220\u0026quot;), fg_params = list(col = \u0026quot;white\u0026quot;, fontsize = 12) ), rowhead = list(), default = list(), padding = unit(c(2, 3), \u0026quot;mm\u0026quot;) ) infin_list \u0026lt;- dat_infin %\u0026gt;% ungroup() %\u0026gt;% filter(n \u0026gt; 5) %\u0026gt;% arrange(desc(mean_claps)) %\u0026gt;% select(infin, n, mean_claps) %\u0026gt;% mutate(mean_claps = round(mean_claps)) %\u0026gt;% rename(\u0026quot;infinitive\u0026quot; = \u0026quot;infin\u0026quot;, \u0026quot;times used\u0026quot; = \u0026quot;n\u0026quot;, \u0026quot;mean claps\u0026quot; = \u0026quot;mean_claps\u0026quot;) %\u0026gt;% head(10) %\u0026gt;% tableGrob(rows = NULL, theme = gtable_theme ) verb_list \u0026lt;- dat_verb %\u0026gt;% ungroup() %\u0026gt;% filter(n \u0026gt; 5) %\u0026gt;% arrange(desc(mean_claps)) %\u0026gt;% select(word, n, mean_claps) %\u0026gt;% mutate(mean_claps = round(mean_claps)) %\u0026gt;% rename(\u0026quot;verb\u0026quot; = \u0026quot;word\u0026quot;, \u0026quot;times used\u0026quot; = \u0026quot;n\u0026quot;, \u0026quot;mean claps\u0026quot; = \u0026quot;mean_claps\u0026quot;) %\u0026gt;% head(10) %\u0026gt;% tableGrob(rows = NULL, theme = gtable_theme) plot(infin_list)  plot(verb_list)  break vs build dat_bb \u0026lt;- dat %\u0026gt;% unnest_tokens(infin, title, token = \u0026quot;ngrams\u0026quot;, n = 2, drop = FALSE) %\u0026gt;% filter(str_detect(pattern = \u0026quot;to \u0026quot;, string = infin)) %\u0026gt;% filter(infin %in% c(\u0026quot;to break\u0026quot;, \u0026quot;to disrupt\u0026quot;, \u0026quot;to destroy\u0026quot;, \u0026quot;to create\u0026quot;, \u0026quot;to make\u0026quot;, \u0026quot;to build\u0026quot;)) %\u0026gt;% mutate( type = case_when( infin %in% c(\u0026quot;to break\u0026quot;, \u0026quot;to disrupt\u0026quot;, \u0026quot;to destroy\u0026quot;) ~ \u0026quot;break\u0026quot;, TRUE ~ \u0026quot;build\u0026quot; ) )  topics / tags plot plot_topic \u0026lt;- dat_bb %\u0026gt;% gather(tag, tag_true, contains(\u0026quot;tag_\u0026quot;)) %\u0026gt;% mutate(tag = str_remove(tag, \u0026quot;tag_\u0026quot;)) %\u0026gt;% group_by(type, tag) %\u0026gt;% summarize( tag_prop = mean(tag_true), tag_sd = sd(tag_true), tag_se = tag_sd/sqrt(n()) ) %\u0026gt;% ggplot(aes( x = factor(tag) %\u0026gt;% reorder(tag_prop, FUN = mean), y = tag_prop, fill = type) ) + geom_col(position=\u0026quot;dodge\u0026quot;, stat=\u0026quot;identity\u0026quot;) + scale_fill_manual(values = c(pink, olive), labels = c(\u0026quot;posts about breaking\\nor disrupting\u0026quot;, \u0026quot;posts about building\\nor creating\u0026quot;), guide = guide_legend(title = NULL)) + geom_errorbar(aes(ymin = tag_prop - tag_se, ymax = tag_prop + tag_se), position = position_dodge(0.9), size = 0.3, width = 0.5, color = \u0026quot;white\u0026quot; ) + xlab(\u0026quot;tag on post\u0026quot;)+ ylab(\u0026quot;proportion with tag\u0026quot;) + theme_merb() + theme( axis.text.x = element_text(hjust = .75, angle = 10) )  ## Warning: Ignoring unknown parameters: stat  plot_topic  time to (ha)rvest some posts! read_post \u0026lt;- function(url){ tryCatch( read_html(url, options = \u0026quot;NOERROR\u0026quot;) %\u0026gt;% html_nodes(\u0026quot;.graf--p\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_flatten(\u0026quot; \u0026quot;), error = function(e){NA}, warning = function(w){NA} ) } dat_bb \u0026lt;- dat_bb %\u0026gt;% mutate( full_text = map(url, read_post) ) dat_bb \u0026lt;- unnest(dat_bb) ## don't want to have to repeat that later, so save it now save(dat_bb, file = \u0026quot;2018-12-04_scraped_posts.RData\u0026quot;)  sentiment plot Not sure if stemming is needed here, given the way the sentiment dictionaries are constructed . . .\nsent \u0026lt;- get_sentiments(\u0026quot;loughran\u0026quot;) dat_sent \u0026lt;- dat_bb %\u0026gt;% filter(!is.na(full_text)) %\u0026gt;% unnest_tokens(word, full_text) %\u0026gt;% left_join(sent)  ## Joining, by = \u0026quot;word\u0026quot;  plot_sent \u0026lt;- dat_sent %\u0026gt;% group_by(x1, sentiment, type) %\u0026gt;% tally() %\u0026gt;% group_by(x1) %\u0026gt;% mutate( proportion = n / sum(n) ) %\u0026gt;% filter(!is.na(sentiment)) %\u0026gt;% group_by(type, sentiment) %\u0026gt;% summarise( median_prop = median(proportion), sd = sd(proportion), se = (1.2533 * sd) / sqrt(n())) %\u0026gt;% ggplot(aes(x = factor(sentiment) %\u0026gt;% reorder(median_prop), y = median_prop, fill = type)) + geom_col(position = \u0026quot;dodge\u0026quot;) + scale_fill_manual(values = c(pink, olive)) + geom_errorbar(aes(ymin = median_prop - se, ymax = median_prop + se), position = position_dodge(0.9), size = 0.3, width = 0.5, color = \u0026quot;white\u0026quot; ) + xlab(\u0026quot;sentiment\u0026quot;)+ ylab(\u0026quot;proportion of words in with sentiment\u0026quot;) + theme_merb() + theme(axis.text.x = element_text(angle = 15, hjust = .75)) plot_sent  violin plot plot_violin \u0026lt;- dat_bb %\u0026gt;% ggplot(aes(x = type, y= reading_time, fill = type, color = type)) + geom_violin(alpha = 0.85) + coord_cartesian(ylim = c(0,10)) + #geom_jitter(alpha = 0.3) + ## too busy stat_summary(fun.y = mean, fun.ymin = function(x){ quantile(x, probs = 0.25) }, fun.ymax = function(x){ quantile(x, probs = 0.75) }, size = 0.75, color = \u0026quot;black\u0026quot; ) + xlab(\u0026quot;post theme\u0026quot;) + ylab(\u0026quot;time to read post\u0026quot;) + scale_fill_manual(values = c(pink, olive)) + scale_color_manual(values = c(pink, olive)) + theme_merb() + theme(#legend.position = \u0026quot;none\u0026quot;, axis.text = element_text(size = 15), axis.title = element_text(size = 20)) plot_violin  legend plot_leg \u0026lt;- grid.arrange(cowplot::get_legend(plot_topic), cowplot::get_legend(plot_point_infin), widths = c(15, 85), layout_matrix = rbind(c(3, 2), c(3, 1)) )  text boxes header header \u0026lt;- ggplot() + annotate(geom = \u0026quot;text\u0026quot;, y = 3.25, x = 1, size = 10, hjust = .8, family = \u0026quot;Roboto\u0026quot;, color = colors_merb_seq[2], label = \u0026quot;what the use of\u0026quot; ) + annotate(geom = \u0026quot;text\u0026quot;, y = 2, x= 1, size = 30, family = \u0026quot;Share Tech Mono\u0026quot;, color = pink, alpha = 0.05, hjust = 0.5, label=\u0026quot;infinitives\u0026quot; ) + annotate(geom = \u0026quot;text\u0026quot;, y = 2, x= 1, size = 15, family = \u0026quot;Share Tech Mono\u0026quot;, color = colors_merb_seq[5], hjust = 0.5, label=\u0026quot;infinitives\u0026quot; ) + annotate(geom = \u0026quot;text\u0026quot;, y = 1, x = 1, size = 10, family = \u0026quot;Roboto\u0026quot;, color = colors_merb_seq[2], hjust = .2, label = \u0026quot;says about a Medium post\u0026quot; ) + ylim(0,4) + theme_void() + theme( plot.background = element_rect(color = \u0026quot;#191919\u0026quot;, fill = olive, size = 5) ) header  definition sidebar def_sidebar \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 1.5, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Roboto\u0026quot;, size = 20, alpha = 0.1, color = \u0026quot;#D9B0AC\u0026quot;, label = \u0026quot;infinitive\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = -2, y = -1, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Roboto\u0026quot;, size = 20, alpha = 0.1, color = \u0026quot;#D9B0AC\u0026quot;, label = \u0026quot;Medium\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = -1.5, y = 2, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Roboto\u0026quot;, size = 4.8, color = \u0026quot;white\u0026quot;, label = paste0( str_wrap(\u0026quot;infinitive: a verb form found in many languages that functions as a noun or is used with auxiliary verbs, and that names the action or state without specifying the subject: “to come”, “to be”, “to want”\u0026quot;, 29), \u0026quot;\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot;\\n\u0026quot;, str_wrap(\u0026quot;Medium: an online publishing platform developed as a long-form alternative to twitter.\u0026quot;, 29)) ) + theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#222220\u0026quot; , color = \u0026quot;#191919\u0026quot;, size = 2) ) def_sidebar  top sidebar top_sidebar \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 2, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Roboto\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;, label = paste0( str_wrap(\u0026quot;Infinitives are used to express wants and desires. When an author uses an infinitive in the title of an article, they are making a promise to the reader about what the reader will learn: 'to build', 'to solve', 'to disrupt'. The figure to the left shows the relationship between how often an infinitive is used in the titles of Medium posts (on the topics of data science) and the number of claps (likes) the post received.\u0026quot;, 32), \u0026quot;\\n\u0026quot;,\u0026quot;\\n\u0026quot;, str_wrap(\u0026quot;Compared to verbs alone, infinitives say a lot more about the content of an article, which you can see in the lists below. They show the 'most-clapped' verbs or infinitives that are found in at least 5 article titles:\u0026quot;, 32) ) )+ theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#20261d\u0026quot; , color = \u0026quot;#191919\u0026quot;, size = 2) ) top_sidebar  bottom sidebar bot_sidebar \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 2, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Share Tech Mono\u0026quot;, size = 6, color = \u0026quot;#D9B0AC\u0026quot;, label = \u0026quot;build vs break\u0026quot; ) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 1.5, hjust = 0, vjust = 1, lineheight = 0.75, family = \u0026quot;Roboto\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;, label = paste0( str_wrap(\u0026quot;Infinitives like 'build' and 'create' are overused in Medium post titles, and have little impact, while 'disrupt' and 'break' see far more claps on average. Comparing these types of posts one can see that 'breaking' posts are usually shorter (a), have more emotional content (b), and are less likely to have highly specific tags (c)\u0026quot;, 32) ) ) + theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#222220\u0026quot; , color = \u0026quot;#191919\u0026quot;, size = 2) ) bot_sidebar  footer footer \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = 0, y = 0, hjust = .5, vjust = .5, lineheight = 0.75, family = \u0026quot;Roboto\u0026quot;, size = 4, color = \u0026quot;#191919\u0026quot;, label = paste0( \u0026quot;A #TidyTuesday adventure + Data from Kaggle.com + Design and and analysis by @TannerKoomar\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot;Code at https://github.com/tkoomar/tidytuesday/blob/master/work/2018-12-04.md\u0026quot; ) ) + theme_void() + theme( plot.background = element_rect(fill = olive , color = \u0026quot;#191919\u0026quot;, size = 2) ) footer  stitch it together png(\u0026quot;2018-12-04_final_plot.png\u0026quot;, width = 1200, height = 1600, res = 144, bg = \u0026quot;#191919\u0026quot;) grid.arrange(header, def_sidebar, plot_point_infin + theme(legend.position = 'none'), top_sidebar, plot_leg, infin_list, verb_list, plot_violin + theme(legend.position = 'none') + labs(tag = \u0026quot;(a)\u0026quot;), plot_sent + theme(legend.position = 'none') + labs(tag = \u0026quot;(b)\u0026quot;), plot_topic + theme(legend.position = 'none')+ labs(tag = \u0026quot;(c)\u0026quot;), bot_sidebar, footer, heights = c(15, 30, 20, 30, 30, 5), layout_matrix = rbind(c(1,1,1,1), c(2,3,3,4), c(5,3,3,4), c(11,9,9,6), c(8,10,10,7), c(12,12,12,12)) ) dev.off()  ## png ## 2  The final Plot ","date":1546646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546646400,"objectID":"6775d487678424ef4ecc976043fc09b4","permalink":"/post/2019-01-05-tt-medium/","publishdate":"2019-01-05T00:00:00Z","relpermalink":"/post/2019-01-05-tt-medium/","section":"post","summary":"Some light web-scraping and natual language processing reveals how different parts of speech may hint at an author\u0026rsquo;s intent (and impact).\n","tags":["R","nlp","web scraping"],"title":"Tidy Tuesday: Infinitives in Medium Posts","type":"post"},{"authors":["Tanner Koomar"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"7b26a5827b9a16a17ce7db4094b86595","permalink":"/publication/2018-case-study-asd/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/2018-case-study-asd/","section":"publication","summary":"In this family-bsed case study, we discovered elevated elevatd levels of common *and* rare variant polygenic risk via whole genome sequencing.","tags":["autism","whole genome sequencing","polygenic risk"],"title":"Whole-genome sequencing in a family with twin boys with autism and intellectual disability suggests multimodal polygenic risk","type":"publication"},{"authors":null,"categories":["tidytuesday"],"content":"\ndat \u0026lt;- read_csv(\u0026quot;https://github.com/rfordatascience/tidytuesday/raw/master/data/2018/2018-11-20/thanksgiving_meals.csv\u0026quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## id = col_double() ## ) ## See spec(...) for full column specifications. Boy is this data messy! Lets relabel and organize it a bit.\ndat \u0026lt;- dat %\u0026gt;% filter(is.na(dat) %\u0026gt;% rowSums() \u0026lt; 50) %\u0026gt;% select(-id, -pie13, -dessert11, -side15, -contains(\u0026quot;Other\u0026quot;)) %\u0026gt;% mutate( \u0026quot;number of\\nkinds of pie\u0026quot; = select(., contains(\u0026quot;pie\u0026quot;)) %\u0026gt;% {!is.na(.)} %\u0026gt;% rowSums(), \u0026quot;total number\\nof sides\u0026quot; = select(., contains(\u0026quot;side\u0026quot;)) %\u0026gt;% {!is.na(.)} %\u0026gt;% rowSums(), \u0026quot;number of\\n non-pie desserts\u0026quot; = select(., contains(\u0026quot;dessert\u0026quot;)) %\u0026gt;% {!is.na(.)} %\u0026gt;% rowSums() ) %\u0026gt;% mutate( family_income = factor(family_income, levels = c(\u0026quot;$0 to $9,999\u0026quot; , \u0026quot;$10,000 to $24,999\u0026quot;, \u0026quot;$25,000 to $49,999\u0026quot;, \u0026quot;$50,000 to $74,999\u0026quot;, \u0026quot;$75,000 to $99,999\u0026quot;, \u0026quot;$100,000 to $124,999\u0026quot;, \u0026quot;$150,000 to $174,999\u0026quot;, \u0026quot;$175,000 to $199,999\u0026quot;, \u0026quot;$200,000 and up\u0026quot;, \u0026quot;Prefer not to answer\u0026quot;, \u0026quot;NA\u0026quot;), ordered = T) ) Odds Ratio plot_or \u0026lt;- dat %\u0026gt;% transmute( urban = community_type == \u0026quot;Urban\u0026quot;, not_urban = community_type != \u0026quot;Urban\u0026quot;, parade = !is.na(watch_program), under30 = dat$age == \u0026quot;18 - 29\u0026quot;, over60 = dat$age == \u0026quot;60+\u0026quot;, female = dat$gender == \u0026quot;Female\u0026quot;, pray = dat$prayer == \u0026quot;Yes\u0026quot; ) %\u0026gt;% #select(prayer, female, friendsgiving, urban, not_urban, parade, under30, over60) %\u0026gt;% map(function(x){ out \u0026lt;- table(gravy = dat$gravy, foo = x) %\u0026gt;% fisher.test(conf.level = 0.6827) out \u0026lt;- c(out$estimate, lower = out$conf.int[1], upper = out$conf.int[2]) return(out) }) %\u0026gt;% as.data.frame() %\u0026gt;% rownames_to_column() %\u0026gt;% gather(key, value, -rowname) %\u0026gt;% spread(rowname, value) %\u0026gt;% ggplot(aes(y = key, x = `odds ratio`))+ geom_errorbarh(aes(xmin = lower, xmax = upper), size = .45, color = \u0026quot;#899DA4\u0026quot;, height = 0.75) + geom_point(size = 4, color = \u0026quot;#DC863B\u0026quot;) + geom_vline(xintercept = 1, lty = 2, lwd = 1, color = \u0026quot;#C93312\u0026quot;) + scale_x_continuous( sec.axis = sec_axis(~ ., breaks = c(0.65, 1.8), labels = c(\u0026quot;less likely\\nto have gravy\u0026quot;, \u0026quot;more likely\\nto have gravy\u0026quot;)) ) + scale_y_discrete(labels = c(\u0026quot;pray\u0026quot; = \u0026quot;pray at dinner\u0026quot;, \u0026quot;urban\u0026quot; = \u0026quot;city dwellers\u0026quot;, \u0026quot;not_urban\u0026quot; = \u0026quot;suburban or\\ncountry dwellers\u0026quot;, \u0026quot;female\u0026quot; = \u0026quot;women\u0026quot;, \u0026quot;under30\u0026quot; = \u0026quot;people under 30\u0026quot;, \u0026quot;over60\u0026quot; = \u0026quot;people over 60\u0026quot;, \u0026quot;parade\u0026quot; = \u0026quot;watch the\\nMacy\u0026#39;s parade\u0026quot;) ) + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;odds ratio\u0026quot;) + theme_minimal() + theme( text = element_text(family = \u0026quot;Poppins\u0026quot;), axis.text.x.top = element_text(size = 15, lineheight = 0.75), axis.text.y = element_text(size = 10, lineheight = 0.7), axis.ticks.x.top = element_blank(), plot.background = element_rect(color = NA, fill = \u0026quot;#fcf7e8\u0026quot;), panel.background = element_rect(color = NA, \u0026quot;#faefd1\u0026quot;), panel.grid = element_line(color = \u0026quot;#fcf7e8\u0026quot;), panel.grid.minor = element_blank() ) plot_or  Violin Plots plot_totals \u0026lt;- dat %\u0026gt;% filter(!is.na(gravy)) %\u0026gt;% select(contains(\u0026quot;number\u0026quot;), gravy) %\u0026gt;% gather(key, value, -gravy) %\u0026gt;% mutate(gravy = case_when( gravy == \u0026quot;Yes\u0026quot; ~ \u0026quot;gravy\u0026quot;, gravy == \u0026quot;No\u0026quot; ~ \u0026quot;no gravy\u0026quot; )) %\u0026gt;% ggplot(aes(x = gravy, y = value, fill = gravy)) + scale_fill_manual(values = c(\u0026quot;gravy\u0026quot; = \u0026quot;#DC863B\u0026quot;, \u0026quot;no gravy\u0026quot; = \u0026quot;#F8AFA8\u0026quot;)) + geom_violin(alpha = 0.75, color = NA) + stat_summary(fun.y = mean, fun.ymin = function(x){ quantile(x, probs = 0.25) }, fun.ymax = function(x){ quantile(x, probs = 0.75) }, color = \u0026quot;#74A089\u0026quot;, size = 0.5 ) + facet_wrap(~ key) + theme_minimal() + theme(legend.position = \u0026#39;none\u0026#39;, text = element_text(family = \u0026quot;Poppins\u0026quot;), strip.text = element_text(size = 12, lineheight = 0.75), axis.text.x.top = element_text(size = 15), axis.text.x = element_text(size = 12), axis.title.x = element_blank(), axis.ticks.x.top = element_blank(), axis.text.y = element_blank(), axis.title.y = element_blank(), plot.background = element_rect(color = NA, fill = \u0026quot;#fcf7e8\u0026quot;), panel.background = element_rect(color = NA, \u0026quot;#faefd1\u0026quot;), panel.grid = element_line(color = \u0026quot;#fcf7e8\u0026quot;), panel.grid.minor = element_blank() ) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.ymin` is deprecated. Use `fun.min` instead. ## Warning: `fun.ymax` is deprecated. Use `fun.max` instead. plot_totals  Line Chart of Income plot_income \u0026lt;- dat %\u0026gt;% filter(family_income != \u0026quot;NA\u0026quot; \u0026amp; family_income != \u0026quot;Prefer not to answer\u0026quot;) %\u0026gt;% group_by(family_income) %\u0026gt;% summarize( gravy = mean(gravy == \u0026quot;Yes\u0026quot;, na.rm = T), gravy_sd = sqrt(gravy*(1-gravy)/n()) ) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(y = gravy, x = family_income, group = 1, color = family_income)) + geom_line(size = 3) + geom_point(size = 6) + geom_errorbar(aes(ymin = gravy - gravy_sd, ymax = gravy + gravy_sd), size = .25, width = .15) + scale_color_manual(values = wesanderson::wes_palette(\u0026quot;Royal2\u0026quot;, n = 9, type = \u0026quot;c\u0026quot;) ) + scale_x_discrete(breaks = c(\u0026quot;$0 to $9,999\u0026quot; ,\u0026quot;$75,000 to $99,999\u0026quot;, \u0026quot;$200,000 and up\u0026quot;), labels = c(\u0026quot;$0 to\\n$9,999\u0026quot; ,\u0026quot;$75,000 to\\n$99,999\u0026quot;, \u0026quot;$200,000\\n and up\u0026quot;)) + ggtitle(\u0026quot;annual family income\u0026quot;) + ylab(\u0026quot;probability of having gravy\u0026quot;) + theme_minimal() + theme( legend.position = \u0026#39;none\u0026#39;, text = element_text(family = \u0026quot;Poppins\u0026quot;), axis.text.x = element_text(size = 12), axis.title.x = element_blank(), axis.ticks.x.top = element_blank(), axis.text.y = element_text(size = 12), axis.title.y = element_text(size = 15), plot.background = element_rect(color = NA, fill = \u0026quot;#fcf7e8\u0026quot;), panel.background = element_rect(color = NA, fill = \u0026quot;#faefd1\u0026quot;), panel.grid = element_line(color = \u0026quot;#fcf7e8\u0026quot;), panel.grid.minor = element_blank(), plot.title = element_text(size = 15, hjust = 0.5) )  ## `summarise()` ungrouping output (override with `.groups` argument) plot_income  Text Boxes header \u0026lt;- ggplot() + annotate(geom = \u0026quot;text\u0026quot;, y = 3.15, x = 2, size = 50, family = \u0026quot;Pacifico\u0026quot;, color = \u0026quot;#fcf7e8\u0026quot;, label=\u0026quot;gravy\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, y = 3.25, x = 2, size = 5, family = \u0026quot;Poppins\u0026quot;, color = \u0026quot;#74A089\u0026quot;, label = \u0026quot;what your thanksgiving\u0026quot; ) + annotate(geom = \u0026quot;text\u0026quot;, y = 2.5, x= 2, size = 25, family = \u0026quot;Pacifico\u0026quot;, color = \u0026quot;#DC863B\u0026quot;, label=\u0026quot;gravy\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, y = .35, x = 2, size = 5,family = \u0026quot;Poppins\u0026quot;, color = \u0026quot;#74A089\u0026quot;, hjust = .6, label = \u0026quot;says about you\u0026quot;) + ylim(0,4) + theme_void() + theme( plot.background = element_rect(color = \u0026quot;#fcf7e8\u0026quot;, fill = \u0026quot;#faefd1\u0026quot;, size = 5) ) header midbar1 \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = -2, hjust = 0, vjust = 0, lineheight = 0.75, family = \u0026quot;Poppins\u0026quot;, size = 4, color = \u0026quot;#899DA4\u0026quot;, label = str_wrap(\u0026quot;A dinner accompanied by congealed broth is a symbol of status and abundance. Below, the presence of gravy at a Thanksgiving meal goes hand-in-hand with a greater variety of both side dishes and pies.\u0026quot;, 70) ) + theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#fcf7e8\u0026quot;, color = NA) ) midbar2 \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = -2, hjust = 0, vjust = 0, lineheight = 0.75, family = \u0026quot;Poppins\u0026quot;, size = 4, color = \u0026quot;#899DA4\u0026quot;, label = str_wrap(\u0026quot;Eating concentrated meat juice is also a luxurious marker of wealth. The probability that a family will have gravy at a Thanksgiving meal increases markedly with annual income.\u0026quot;, 70)) + theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#fcf7e8\u0026quot;, color = NA) ) midbar1  midbar2 sidebar \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = -2, y = 2, hjust = 0, vjust = 1, lineheight = 1.05, family = \u0026quot;Poppins\u0026quot;, size = 4.4, color = \u0026quot;#899DA4\u0026quot;, label = str_wrap(\u0026quot;Gravy is truly the bellwether of the thanksgiving table. If you abstain from gravy, you are much more likely to live in a city, identify as a woman, and be younger than 30. Gravy-eaters, on the other hand, are much more likely to have an AARP membership, watch the Thanksgiving day parade, pray before dinner, and live outside or an urban center.\u0026quot;, 29)) + theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#faefd1\u0026quot;, color = NA) ) sidebar footnote \u0026lt;- ggplot() + ylim(-2,2) + xlim(-2,2) + annotate(\u0026quot;text\u0026quot;, x = 2, y = -2, hjust = 1, vjust = 0, lineheight = 0.75, family = \u0026quot;Poppins\u0026quot;, size = 2.75, color = \u0026quot;#899DA4\u0026quot;, label = paste(\u0026quot;a #TidyTuesday adventure\\ndesign by @TannerKoomar\\ndata from FiveThirtyEight\\ncolor scheme from karthik/wesanderson\u0026quot;)) + theme_void() + theme( plot.background = element_rect(fill = \u0026quot;#fcf7e8\u0026quot;, color = NA) ) footnote  Plots Assemble! final_plot \u0026lt;- grid.arrange(header, midbar1, midbar2, ggplotGrob(plot_totals), ggplotGrob(plot_income), ggplotGrob(plot_or), sidebar, footnote, heights = c(.15, .075, .35, .025, .325, .075), layout_matrix = rbind(c(1,1,1,1), c(2,2,3,3), c(4,4,5,5), c(NA,NA,NA,NA), c(6,6,6,7), c(6,6,6,8)) ) ## Get rid of that ugly white bar in the middle. . . cowplot::ggdraw(final_plot) + theme(plot.background = element_rect(fill = \u0026quot;#fcf7e8\u0026quot;))  ","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"37ba72e2c29a7fd786a8cc8423a967b9","permalink":"/post/2018-11-20-tt-gravy/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/post/2018-11-20-tt-gravy/","section":"post","summary":" Learn what your gravy eating habits say about you with a fun fall color scheme from the wesanderson package. ","tags":["R","holiday","gridExtra"],"title":"Tidy Tuesday: Thanksgiving Gravy","type":"post"},{"authors":null,"categories":["tidytuesday"],"content":"Load Data dat \u0026lt;- read_csv(\u0026quot;https://github.com/rfordatascience/tidytuesday/raw/master/data/2018/2018-11-06/us_wind.csv\u0026quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## faa_ors = col_character(), ## faa_asn = col_character(), ## t_state = col_character(), ## t_county = col_character(), ## t_fips = col_character(), ## p_name = col_character(), ## t_manu = col_character(), ## t_model = col_character(), ## t_img_date = col_character(), ## t_img_srce = col_character() ## ) ## See spec(...) for full column specifications. ## get all the missing data to NA dat \u0026lt;- dat %\u0026gt;% mutate_all(.funs = funs(replace(., . %in% c(-9999, \u0026quot;missing\u0026quot;), NA))) ## Warning: `funs()` is deprecated as of dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## drop rows and columns with high missing dat.comp \u0026lt;- dat %\u0026gt;% select_if( function(x){sum(is.na(x))/length(x) \u0026lt; .15}) %\u0026gt;% filter(rowMeans(is.na(.)) \u0026lt; 0.15) ## get info at project level dat.p.num \u0026lt;- dat.comp %\u0026gt;% group_by(p_name) %\u0026gt;% select_if(is.numeric) %\u0026gt;% select(-case_id) %\u0026gt;% summarise_all(.funs = function(x){mean(x, na.rm = T)}) %\u0026gt;% ungroup %\u0026gt;% select(-p_name) %\u0026gt;% filter(rowSums(is.na(.)) == 0) %\u0026gt;% filter(xlong \u0026lt; 100 \u0026amp; xlong \u0026gt; -125 \u0026amp; ylat \u0026gt; 20) %\u0026gt;% ## remove some geographic outliers filter(t_cap \u0026gt; 0 \u0026amp; t_cap \u0026lt; 6000)   Reduce Dimensionality Do a PCA Looks like PC1 makes up the bul of the difference, and it is due to turbine rotor sweep area\ndat.pca \u0026lt;- prcomp(dat.p.num, center = TRUE, scale. = TRUE) summary(dat.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.3420 1.3762 1.00832 0.98379 0.94979 0.86079 0.7040 ## Proportion of Variance 0.4571 0.1578 0.08473 0.08065 0.07517 0.06175 0.0413 ## Cumulative Proportion 0.4571 0.6149 0.69963 0.78028 0.85546 0.91720 0.9585 ## PC8 PC9 PC10 PC11 PC12 ## Standard deviation 0.53133 0.35791 0.27616 0.10600 0.01020 ## Proportion of Variance 0.02353 0.01067 0.00636 0.00094 0.00001 ## Cumulative Proportion 0.98202 0.99270 0.99905 0.99999 1.00000 dat.pca$rotation %\u0026gt;% as.data.frame() %\u0026gt;% rownames_to_column() %\u0026gt;% ggplot(aes(y = PC1, x = rowname)) + geom_col()  Some plots dat.p.num %\u0026gt;% bind_cols(as.tibble(dat.pca$x)) %\u0026gt;% ggplot(aes(x = xlong, ylat, color = PC1, size = p_cap, alpha = t_cap)) + scale_color_viridis_c() + geom_point() ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated.  Cluster ## Use the PCs that explain 95% of the variance dat.k \u0026lt;- dat.pca$x %\u0026gt;% as.tibble %\u0026gt;% select(1:7) %\u0026gt;% mutate( k2 = kmeans(., 2)$cluster, k3 = kmeans(., 3)$cluster, k4 = kmeans(., 4)$cluster, k5 = kmeans(., 5)$cluster, k6 = kmeans(., 6)$cluster ) %\u0026gt;% bind_cols(dat.p.num)  Cluster Plots dat.k %\u0026gt;% ggplot(aes(x = xlong, ylat, color = as.character(k6), size = p_cap, alpha = t_cap)) + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;) + geom_point() dat.k %\u0026gt;% ggplot(aes(x = xlong, ylat, color = as.character(k6), size = p_cap, alpha = t_cap)) + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;)+ geom_point() + facet_wrap(~ as.character(k6), nrow = 3)   Make a pretty map under the data library(ggmap) invert \u0026lt;- function(M) { i \u0026lt;- function(x){rgb(t(255-col2rgb(x))/255)} m \u0026lt;- M %\u0026gt;% apply(2, i) %\u0026gt;% as.raster() class(m) \u0026lt;- class(M) attr(m, \u0026quot;bb\u0026quot;) \u0026lt;- attr(M, \u0026quot;bb\u0026quot;) return(m) } us \u0026lt;- c(left = -125, bottom = 25.75, right = -67, top = 49) m \u0026lt;- get_stamenmap(us, zoom = 5, maptype = \u0026quot;toner-lite\u0026quot;) ggmap(invert(m))  Plots plot.points \u0026lt;- ggmap(invert(m), extent = \u0026#39;device\u0026#39;) + geom_point(aes(x = xlong, y = ylat, color = as.character(k6), size = p_cap, alpha = t_cap), data = dat.k, pch = 18) + scale_color_brewer(palette = \u0026quot;Set1\u0026quot;) + facet_wrap(~k6, ncol = 1) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;, strip.text = element_blank()) + theme(plot.background = element_rect(fill = \u0026quot;black\u0026quot;, color = \u0026quot;black\u0026quot;), panel.background = element_rect(fill = NULL)) plot.density \u0026lt;- ggmap(invert(m), extent = \u0026#39;device\u0026#39;) + stat_density2d(aes(x = xlong, y = ylat, alpha = ..level.., fill = as.character(k6), color = NULL), data = dat.k, geom = \u0026quot;polygon\u0026quot;) + scale_fill_brewer(palette = \u0026quot;Set1\u0026quot;) + facet_wrap(~k6, ncol = 1) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;, strip.text = element_blank()) + theme(plot.background = element_rect(fill = \u0026quot;black\u0026quot;, color = \u0026quot;black\u0026quot;), panel.background = element_rect(fill = NULL)) plot.all \u0026lt;- ggmap(invert(m), extent = \u0026#39;device\u0026#39;) + geom_point(aes(x = xlong, y = ylat, color = as.character(k6), size = p_cap, alpha = t_cap), data = dat.k, pch = 18) + scale_color_brewer(palette = \u0026quot;Set1\u0026quot;) + guides( size = guide_legend(title = \u0026quot;project capacity\u0026quot;,title.position = \u0026#39;top\u0026#39;, override.aes = list(color = \u0026quot;white\u0026quot;)), alpha = guide_legend(title = \u0026quot;turbine capacity\u0026quot;,title.position = \u0026#39;top\u0026#39;, override.aes = list(color = \u0026quot;white\u0026quot;, size = 4)), color = guide_legend(title = \u0026#39;cluster\u0026#39;, title.position = \u0026#39;top\u0026#39;, override.aes = list(size = 4)) ) + ggtitle(\u0026quot;US Wind Turbine Projects\u0026quot;, \u0026quot; a #tidytuesday adventure\u0026quot;) + theme_void() + theme(title = element_text(color = \u0026#39;white\u0026#39;), legend.position = \u0026#39;bottom\u0026#39;, legend.background = element_blank(), legend.text = element_text(color = \u0026quot;white\u0026quot;), legend.title = element_text(color = \u0026quot;white\u0026quot;), legend.key = element_blank(), axis.text = element_blank(), axis.title = element_blank(), plot.background = element_rect(fill = \u0026quot;black\u0026quot;, color = \u0026quot;black\u0026quot;), panel.background = element_rect(fill = NULL)) Squish together library(cowplot) ## ## ******************************************************** ## Note: As of version 1.0.0, cowplot does not change the ## default ggplot2 theme anymore. To recover the previous ## behavior, execute: ## theme_set(theme_cowplot()) ## ******************************************************** ## ## Attaching package: \u0026#39;cowplot\u0026#39; ## The following object is masked from \u0026#39;package:ggmap\u0026#39;: ## ## theme_nothing final.plot \u0026lt;- plot_grid(plotlist = list(plot.points, plot.all, plot.density), nrow = 1, rel_widths = c(.25,1,.25)) ggdraw(final.plot) + theme(plot.background = element_rect(fill = \u0026quot;black\u0026quot;, color = \u0026quot;black\u0026quot;))   ","date":1541462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541462400,"objectID":"343013edc49f39fcf7b4ffae663cfabb","permalink":"/post/2018-11-06-tt-wind-power/","publishdate":"2018-11-06T00:00:00Z","relpermalink":"/post/2018-11-06-tt-wind-power/","section":"post","summary":" This TidyTuesday we explores wind power throughout the USA with a high-contrast compound figure made with the popular cowplot package.\n","tags":["R","PCA","map","cowplot"],"title":"Tidy Tuesday: USA Wind Power","type":"post"},{"authors":["Tanner Koomar"],"categories":null,"content":"","date":1539820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539820800,"objectID":"393e6f1886e18cfa56f08cf02a8af83a","permalink":"/publication/2018-forecasd-preprint/","publishdate":"2018-10-18T00:00:00Z","relpermalink":"/publication/2018-forecasd-preprint/","section":"publication","summary":"A new quantitative scoring method for autism-related genes, integrating muliple modalities of data with a random forest model.","tags":["autism","machine learning","random forest","preprint"],"title":"Forecasting autism gene discovery with machine learning and genome-scale data","type":"publication"},{"authors":null,"categories":null,"content":"Well-powered genetic studies of any complex trait require large cohorts. Most current methods of assessing language ability have their roots in clinical practice, and require one on one interaction between a subject and an evaluator. Furthermore, these tests are formulated to be sensitive to variations at the low end of the ability spectrum, but cannot differentiate between individuals of average or better ability.\nThe study of language genetics needs a rapid, scalable assessment which can detect a wide range of ability levels. We are developing a web-based language screening tool composed of a mixture of established and new assessments with an eye to detecting meaningful variation in the language ability of adults.\nBooth Our first deployment of the language screener was in a soundproof booth in a hospital clinic waiting room. We expect participants to return and take screener multiple times, so a mood survey was included to see how mood might be reflected in performance on the various tasks.\nWhy not an app? Web-based tools do pose challenges based on the wide array of hardware and software available to consumers, but every phone, tablet and personal computer has access to a web browser, lowering barriers for entry that are imposed by apps and other single-platform software.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"cf53058c50fb15a25bc0f64263f7dd32","permalink":"/project/language-screener/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/language-screener/","section":"project","summary":"A web-based and battery of language assessments deployed via AWS.","tags":["language genetics","cloud"],"title":"Web-based Language Screener","type":"project"},{"authors":["Tanner Koomar"],"categories":null,"content":"","date":1481760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481760000,"objectID":"e9b7bcfe319f67fbe19d212d833687dc","permalink":"/publication/2017-cerebroviz-paper/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/2017-cerebroviz-paper/","section":"publication","summary":"An R package designed to use SVG drawings of brains as heatmaps, such as for visualizing gene expression data from BrainSpan","tags":["R","data visualization","gene expression"],"title":"cerebroViz: an R package for anatomical visualization of spatiotemporal brain data","type":"publication"},{"authors":null,"categories":null,"content":"In this NIDCD/NICHD-supported project, we are performing whole-genome sequencing of a cohort of Iowa children with language impairment (collected by UI collaborator Bruce Tomblin), where we hope to identify genes and mutations involved in the acquisition and use of language. We hope that research in this area will eventually help to further refine the categorization of language pathology so that therapies can be more targeted and effective.\n","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"b27b5141245e95927bc5f0efc8297eb5","permalink":"/project/language-wgs/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/project/language-wgs/","section":"project","summary":"Whole genome sequencing of a cohort enriched for language impairment","tags":["language genetics","whole genome sequencing"],"title":"Genetics of Language Ability","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8334c3f4a4358949464f152ed873f108","permalink":"/tidytuesday/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tidytuesday/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a88848674f199fc27d42cdcdd983e4c","permalink":"/tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]